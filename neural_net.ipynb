{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# new:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the data from Lino's preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/X.csv')\n",
    "y = pd.read_csv('data/y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76822 entries, 0 to 76821\n",
      "Columns: 495 entries, age to job_end_2019\n",
      "dtypes: bool(259), float64(236)\n",
      "memory usage: 157.3 MB\n",
      "\n",
      " y info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76822 entries, 0 to 76821\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   sphus   76822 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 600.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"X info:\")\n",
    "X.info()\n",
    "print(\"\\n y info:\")\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of               sphus\n",
       "0           5. Poor\n",
       "1           4. Fair\n",
       "2           4. Fair\n",
       "3           3. Good\n",
       "4      2. Very good\n",
       "...             ...\n",
       "76817       4. Fair\n",
       "76818       4. Fair\n",
       "76819       3. Good\n",
       "76820       4. Fair\n",
       "76821       3. Good\n",
       "\n",
       "[76822 rows x 1 columns]>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, need to convert this to numeric to make it into tensor (later will need to map it back to categorical to get meaningful answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sphus\n",
      "0          4\n",
      "1          3\n",
      "2          3\n",
      "3          2\n",
      "4          1\n",
      "...      ...\n",
      "76817      3\n",
      "76818      3\n",
      "76819      2\n",
      "76820      3\n",
      "76821      2\n",
      "\n",
      "[76822 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y['sphus'] = label_encoder.fit_transform(y['sphus'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6999947931582099 0.14999609486865742 0.15000911197313269\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# train is now 70% of the entire data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_ratio, random_state = 123)\n",
    "\n",
    "# test is now 15% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = test_ratio / (test_ratio + validation_ratio), random_state = 10117)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n = n_train + n_val + n_test\n",
    "\n",
    "print((n_train / n), (n_val / n), (n_test / n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs in train data\n",
      "No Infinities in train data\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in the training data\n",
    "if X_train.isna().any().any() or y_train.isna().any().any():\n",
    "    print(\"NaNs in train data\")\n",
    "else:\n",
    "    print(\"No NaNs in train data\")\n",
    "\n",
    "# Check for infinities in the training data\n",
    "if (X_train == np.inf).any().any() or (y_train == np.inf).any().any():\n",
    "    print(\"Infinities in train data\")\n",
    "else:\n",
    "    print(\"No Infinities in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute & scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTING:\n",
    "\n",
    "# create copies of original\n",
    "X_train_imputed = X_train.copy()\n",
    "X_test_imputed = X_test.copy()\n",
    "X_val_imputed = X_val.copy()\n",
    "\n",
    "# non numeric\n",
    "# Select non-numeric columns (booleans)\n",
    "non_numeric_columns_train = X_train_imputed.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_test = X_test_imputed.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_val = X_val_imputed.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "\n",
    "# Impute non-numeric columns with the string \"missing\"\n",
    "X_train_imputed[non_numeric_columns_train] = X_train_imputed[non_numeric_columns_train].fillna(\"missing\")\n",
    "X_test_imputed[non_numeric_columns_test] = X_test_imputed[non_numeric_columns_test].fillna(\"missing\")\n",
    "X_val_imputed[non_numeric_columns_val] = X_val_imputed[non_numeric_columns_val].fillna(\"missing\")\n",
    "\n",
    "\n",
    "# numeric\n",
    "# Select numeric columns\n",
    "numeric_columns_train = X_train_imputed.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_test = X_test_imputed.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_val = X_val_imputed.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Initialize the imputer for numeric columns\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation to numeric columns in training data (fit & transform)\n",
    "X_train_imputed[numeric_columns_train] = numeric_imputer.fit_transform(X_train_imputed[numeric_columns_train])\n",
    "\n",
    "# Apply imputation to numeric columns in testing / val data using the *same* imputer (only transform)\n",
    "X_test_imputed[numeric_columns_test] = numeric_imputer.transform(X_test_imputed[numeric_columns_test])\n",
    "X_val_imputed[numeric_columns_val] = numeric_imputer.transform(X_val_imputed[numeric_columns_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING\n",
    "\n",
    "# initialise\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training data (fit and transform)\n",
    "X_train_imputed_scaled = X_train_imputed.copy()\n",
    "X_train_imputed_scaled[numeric_columns_train] = scaler.fit_transform(X_train_imputed[numeric_columns_train])\n",
    "\n",
    "# Scale testing & validation data (only transform)\n",
    "X_test_inputed_scaled = X_test_imputed.copy()\n",
    "X_test_inputed_scaled[numeric_columns_test] = scaler.transform(X_test_imputed[numeric_columns_test])\n",
    "\n",
    "X_val_imputed_scaled = X_val_imputed.copy()\n",
    "X_val_imputed_scaled[numeric_columns_test] = scaler.transform(X_val_imputed[numeric_columns_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature DataFrames to NumPy arrays and then to tensors\n",
    "X_train_torch = torch.tensor(X_train_imputed_scaled.to_numpy().astype(np.float32))\n",
    "X_val_torch = torch.tensor(X_val_imputed_scaled.to_numpy().astype(np.float32))\n",
    "X_test_torch = torch.tensor(X_test_inputed_scaled.to_numpy().astype(np.float32))\n",
    "\n",
    "# Convert label Series to tensors and unsqueeze to add an extra dimension\n",
    "y_train_torch = torch.tensor(y_train.to_numpy().astype(np.float32)).unsqueeze(1)\n",
    "y_val_torch = torch.tensor(y_val.to_numpy().astype(np.float32)).unsqueeze(1)\n",
    "y_test_torch = torch.tensor(y_test.to_numpy().astype(np.float32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we're doing classification we do NOT need to scale the y-label series right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader instances\n",
    "\n",
    "# Ensure targets are Long tensors and 1D\n",
    "# needs to be 'long' for categorical, where loss functions that expect integer labels\n",
    "# .view(-1) reshapes a tensor: -1 is placeholder for an unknown dimension which PyTorch infers based on the other dimensions. \n",
    "# # So .view(-1) flattens  tensor into a 1-D tensor.\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch.long().view(-1)) \n",
    "val_dataset = TensorDataset(X_val_torch, y_val_torch.long().view(-1))\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch.long().view(-1))\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below: had errors with targets being out of bound, this just asserts they are all within bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # number of output classes required\n",
    "    num_classes = len(torch.unique(y_train_torch))\n",
    "\n",
    "    # Ensure all target values are within the valid range\n",
    "    assert torch.all((y_train_torch >= 0) & (y_train_torch < num_classes)), \"Train targets out of range\"\n",
    "    assert torch.all((y_val_torch >= 0) & (y_val_torch < num_classes)), \"Validation targets out of range\"\n",
    "    assert torch.all((y_test_torch >= 0) & (y_test_torch < num_classes)), \"Test targets out of range\"\n",
    "    print(\"All OK\")\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write functions for full workflow\n",
    "\n",
    "### 1) A function to train the model over one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, loss_fn, optimizer):\n",
    "    model.train()  \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        \n",
    "        # ensure targets are Long tensor and 1D (already done this, this is robustness check)\n",
    "        targets = targets.long().view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()  # zero previous gradients\n",
    "\n",
    "        # forward pass: compute model output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass: compute gradient of loss wrt to model params\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Return the average loss for this epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) A function to evaluate error on entire val / test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_loader, model, loss_fn):\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "\n",
    "            # ensure targets are Long tensor and 1D (again, already done, but this is robustness check)\n",
    "            targets = targets.long().view(-1)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # aggregate the loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    # return average loss for the entire dataset\n",
    "    average_loss = total_loss / len(data_loader) \n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) A function to integrate training and validation, returning the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(train_loader, val_loader, model, epochs, optimizer, loss_fn):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train the model for one epoch\n",
    "        train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = evaluate_model(val_loader, model, loss_fn)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Check if this is the smallest so far and save associated model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        #print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) A function to get predictions from a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model.eval()  # met model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # again disable gradient computation\n",
    "        for inputs, _ in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            predictions.extend(predicted.cpu().numpy())  # Convert to numpy array and extend the list\n",
    "    \n",
    "    return np.array(predictions)  # Convert the list to a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) A function to produce classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def generate_classification_report(model, test_loader):\n",
    "    predictions = get_predictions(model, test_loader)\n",
    "    \n",
    "    # Extract true labels from test_loader\n",
    "    true_labels = []\n",
    "    for _, targets in test_loader:\n",
    "        true_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Generate and print classification report with zero_division handling\n",
    "    report = classification_report(\n",
    "        true_labels, predictions, target_names=label_encoder.classes_, zero_division=0\n",
    "    )\n",
    "    print(f\"\\n Classification Report for {model.name}:\")\n",
    "    print(report)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) A final workflow function - brings it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow(models, train_loader, val_loader, test_loader, epochs=20):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    results = []\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"\\n Training model: {model.name}\")\n",
    "        \n",
    "        # Define loss function and optimizer \n",
    "        loss_fn = nn.CrossEntropyLoss() # necessary for categorical y variable\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train and validate the model\n",
    "        trained_model, train_losses, val_losses = train_and_validate(\n",
    "            train_loader, val_loader, model, epochs, optimizer, loss_fn\n",
    "        )\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_loss = evaluate_model(test_loader, trained_model, loss_fn)\n",
    "        \n",
    "        classification_report_str = generate_classification_report(trained_model, test_loader)\n",
    "        \n",
    "        results.append({\n",
    "            'model_name': model.__class__.__name__,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'test_loss': test_loss,\n",
    "            'classification_report': classification_report_str\n",
    "        })\n",
    "        \n",
    "        # Track the best model based on validation loss\n",
    "        if min(val_losses) < best_val_loss:\n",
    "            best_val_loss = min(val_losses)\n",
    "            best_model = trained_model\n",
    "        \n",
    "        print(f\"Completed training for model: {model.name}\")\n",
    "\n",
    "    return best_model, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: need to provide following parameters to run_workflow():\n",
    "- models (a list of instantiated models), \n",
    "- train_loader, \n",
    "- val_loader, \n",
    "- test_loader, \n",
    "- (epochs)\n",
    "\n",
    "and it will return:\n",
    "- best_model,\n",
    "- results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic linear, 3 layered (???) NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size  \n",
    "        self.hidden = nn.Linear(input_features, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Model:\n",
    "\n",
    "- Batch normalization helps in normalising  inputs of each layer, reducing internal covariate shift. \n",
    "- Same as before: nonlinear ReLU activation function ensures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.output = nn.Linear(128, output_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout model\n",
    "- after 1st activation layer, using a dropout rate of 50% to prevent overfitting - works by randomly deactivating half of the output units during training. \n",
    "- LeakyReLU: against issue of dying nodes (neurons) through small, positive gradient for negative inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(DropoutNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 500)\n",
    "        self.layer2 = nn.Linear(500, 100)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(100, output_features)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.layer2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combo Model\n",
    "\n",
    "- integrates both dropout regularization and batch normalization across three layers \n",
    "- intended to enhance training stability and prevent overfitting\n",
    "- employs Leaky ReLU as the activation function again to ensure consistent small gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(CombinedNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Moderate dropout\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.output = nn.Linear(128, output_features)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.leaky_relu(self.bn1(self.layer1(x))))\n",
    "        x = self.dropout2(self.leaky_relu(self.bn2(self.layer2(x))))\n",
    "        x = self.leaky_relu(self.bn3(self.layer3(x)))\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Activation Functions\n",
    "\n",
    "- uses different activation functions at different layers to capture different types of nonlinear relationships at various levels of abstraction\n",
    "-  idea is to enable the model to learn more *diverse* representations by varying the transformations applied to inputs / intermediate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedActivationNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(MixedActivationNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.elu = nn.ELU()\n",
    "        self.output_layer = nn.Linear(64, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.leaky_relu(self.layer2(x))\n",
    "        x = self.elu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "\"Employs principles of a Residual Network (ResNet) architecture adapted for regression tasks. It is designed with an initial linear block to project input features into a 64-dimensional space, followed by a series of residual blocks that apply transformations and integrate the original input via skip connections to enhance learning without degradation, culminating with a final linear layer to produce a scalar output\"\n",
    "\n",
    "- main point: residual blocks enable deeper networks by adding shortcut connections that bypass one or more layers, aiming to address the problem of vanishing gradients in deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build module\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_features, output_features)\n",
    "        self.bn1 = nn.BatchNorm1d(output_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear2 = nn.Linear(output_features, output_features)\n",
    "        self.bn2 = nn.BatchNorm1d(output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.linear1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity  \n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# build model\n",
    "class ResNetRegression(nn.Module):\n",
    "    def __init__(self, input_features, block, num_blocks, output_features=1):\n",
    "        super(ResNetRegression, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.input_features = input_features\n",
    "        self.current_features = 64  # here is initial number of features\n",
    "        self.init_block = nn.Sequential(\n",
    "            nn.Linear(input_features, self.current_features),\n",
    "            nn.BatchNorm1d(self.current_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.blocks = self._make_layer(block, 64, num_blocks)\n",
    "        self.final_layer = nn.Linear(64, output_features)\n",
    "\n",
    "    def _make_layer(self, block, output_features, num_blocks):\n",
    "        layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(block(self.current_features, output_features))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_block(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Routing Network\n",
    "\n",
    "idea is a \"routing\" mechanism that dynamically decides how much of each layer's output should contribute to the next layer's input.\n",
    "\n",
    "- uses gating layers to selectively control the flow of information through the network by multiplying the output of a linear transformation of the input (x_transformed) by a gating signal (gate). \n",
    "- gating mechanism is a sigmoid function, which scales the linearly transformed input before passing it through gated layers\n",
    "- as before, residual blocks enable deeper networks by adding shortcut connections that bypass one or more layers, aiming to address the problem of vanishing gradients in deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build module \n",
    "class GatingLayer(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(GatingLayer, self).__init__()\n",
    "        self.layer = nn.Linear(input_features, output_features)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_features, output_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.layer(x)\n",
    "        gate = self.gate(x)\n",
    "        return x_transformed * gate\n",
    "\n",
    "# build model\n",
    "class DynamicRoutingNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(DynamicRoutingNetwork, self).__init__()\n",
    "        self.layer1 = GatingLayer(input_features, 128)\n",
    "        self.layer2 = GatingLayer(128, 64)\n",
    "        self.layer3 = GatingLayer(64, 64)\n",
    "        self.output_layer = nn.Linear(64, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for automatically naming models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_name(models):\n",
    "     model_names = []\n",
    "     for model in models:\n",
    "         try:\n",
    "             name = f\"{model.__class__.__name__} with {model.hidden_size} hidden nodes\"\n",
    "         except AttributeError:\n",
    "             #  where hidden_nodes attribute is not available\n",
    "             name = f\"{model.__class__.__name__}\"\n",
    "        \n",
    "         # set the name attribute\n",
    "         model.name = name\n",
    "         model_names.append(name)\n",
    "     return model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NeuralNetwork with 10 hidden nodes',\n",
       " 'NeuralNetwork with 100 hidden nodes',\n",
       " 'DeepNeuralNetwork',\n",
       " 'DropoutNeuralNetwork',\n",
       " 'CombinedNeuralNetwork',\n",
       " 'MixedActivationNetwork',\n",
       " 'ResNetRegression',\n",
       " 'DynamicRoutingNetwork']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features = X_train_torch.shape[1]\n",
    "output_features = num_classes  \n",
    "\n",
    "base_nn_10 = NeuralNetwork(input_features=input_features, output_features=output_features, hidden_size=10)\n",
    "base_nn_100 = NeuralNetwork(input_features=input_features, output_features=output_features, hidden_size=100)\n",
    "deep_model = DeepNeuralNetwork(input_features=input_features, output_features=output_features)\n",
    "dropout_model = DropoutNeuralNetwork(input_features=input_features, output_features=output_features,)\n",
    "combo_model = CombinedNeuralNetwork(input_features=input_features, output_features=output_features,)\n",
    "mixed_model = MixedActivationNetwork(input_features=input_features, output_features=output_features,)\n",
    "ResNet_model = ResNetRegression(input_features=input_features, output_features=output_features, block=ResidualBlock, num_blocks=3)\n",
    "routing_model = DynamicRoutingNetwork(input_features=input_features, output_features=output_features)\n",
    "\n",
    "models = [\n",
    "    base_nn_10,\n",
    "    base_nn_100,\n",
    "    deep_model,\n",
    "    dropout_model,\n",
    "    combo_model,\n",
    "    mixed_model,\n",
    "    ResNet_model,\n",
    "    routing_model\n",
    "]\n",
    "model_name(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Comparative Training & Validating\n",
    "\n",
    "Reminder: need to provide following parameters to run_workflow():\n",
    "Summary: need to provide following parameters to run_workflow():\n",
    "- models (a list of models) \n",
    "- train_loader, \n",
    "- val_loader, \n",
    "- test_loader, \n",
    "- (epochs)\n",
    "\n",
    "and it will return:\n",
    "- best_model,\n",
    "- results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training model: NeuralNetwork with 10 hidden nodes\n",
      "\n",
      " Classification Report for NeuralNetwork with 10 hidden nodes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.00      0.00      0.00       701\n",
      "2. Very good       0.44      0.39      0.41      1769\n",
      "     3. Good       0.53      0.65      0.58      4226\n",
      "     4. Fair       0.52      0.55      0.53      3368\n",
      "     5. Poor       0.63      0.53      0.58      1460\n",
      "\n",
      "    accuracy                           0.53     11524\n",
      "   macro avg       0.42      0.42      0.42     11524\n",
      "weighted avg       0.49      0.53      0.51     11524\n",
      "\n",
      "Completed training for model: NeuralNetwork with 10 hidden nodes\n",
      "\n",
      " Training model: NeuralNetwork with 100 hidden nodes\n",
      "\n",
      " Classification Report for NeuralNetwork with 100 hidden nodes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.00      0.00      0.00       701\n",
      "2. Very good       0.00      0.00      0.00      1769\n",
      "     3. Good       0.48      0.74      0.58      4226\n",
      "     4. Fair       0.51      0.56      0.54      3368\n",
      "     5. Poor       0.63      0.53      0.58      1460\n",
      "\n",
      "    accuracy                           0.50     11524\n",
      "   macro avg       0.32      0.37      0.34     11524\n",
      "weighted avg       0.40      0.50      0.44     11524\n",
      "\n",
      "Completed training for model: NeuralNetwork with 100 hidden nodes\n",
      "\n",
      " Training model: DeepNeuralNetwork\n",
      "\n",
      " Classification Report for DeepNeuralNetwork:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.45      0.17      0.25       701\n",
      "2. Very good       0.45      0.40      0.43      1769\n",
      "     3. Good       0.53      0.64      0.58      4226\n",
      "     4. Fair       0.52      0.53      0.52      3368\n",
      "     5. Poor       0.63      0.52      0.57      1460\n",
      "\n",
      "    accuracy                           0.53     11524\n",
      "   macro avg       0.52      0.45      0.47     11524\n",
      "weighted avg       0.52      0.53      0.52     11524\n",
      "\n",
      "Completed training for model: DeepNeuralNetwork\n",
      "\n",
      " Training model: DropoutNeuralNetwork\n"
     ]
    }
   ],
   "source": [
    "# Run  !!!\n",
    "best_model, results = run_workflow(models, train_loader, val_loader, test_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NeuralNetwork\n",
      "Test Loss: 1.3721\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.00      0.00      0.00       701\n",
      "2. Very good       0.45      0.35      0.39      1769\n",
      "     3. Good       0.52      0.69      0.59      4226\n",
      "     4. Fair       0.52      0.52      0.52      3368\n",
      "     5. Poor       0.64      0.48      0.54      1460\n",
      "\n",
      "    accuracy                           0.52     11524\n",
      "   macro avg       0.42      0.41      0.41     11524\n",
      "weighted avg       0.49      0.52      0.50     11524\n",
      "\n",
      "Model: NeuralNetwork\n",
      "Test Loss: 1.3743\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.00      0.00      0.00       701\n",
      "2. Very good       0.44      0.39      0.42      1769\n",
      "     3. Good       0.52      0.70      0.59      4226\n",
      "     4. Fair       0.53      0.49      0.51      3368\n",
      "     5. Poor       0.65      0.51      0.57      1460\n",
      "\n",
      "    accuracy                           0.52     11524\n",
      "   macro avg       0.43      0.42      0.42     11524\n",
      "weighted avg       0.49      0.52      0.50     11524\n",
      "\n",
      "Model: DeepNeuralNetwork\n",
      "Test Loss: 1.0937\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.46      0.21      0.29       701\n",
      "2. Very good       0.45      0.39      0.42      1769\n",
      "     3. Good       0.53      0.63      0.58      4226\n",
      "     4. Fair       0.53      0.52      0.52      3368\n",
      "     5. Poor       0.61      0.56      0.58      1460\n",
      "\n",
      "    accuracy                           0.53     11524\n",
      "   macro avg       0.52      0.46      0.48     11524\n",
      "weighted avg       0.52      0.53      0.52     11524\n",
      "\n",
      "Model: DropoutNeuralNetwork\n",
      "Test Loss: 1.0742\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.46      0.23      0.30       701\n",
      "2. Very good       0.51      0.22      0.31      1769\n",
      "     3. Good       0.52      0.69      0.59      4226\n",
      "     4. Fair       0.52      0.57      0.55      3368\n",
      "     5. Poor       0.65      0.52      0.57      1460\n",
      "\n",
      "    accuracy                           0.53     11524\n",
      "   macro avg       0.53      0.44      0.47     11524\n",
      "weighted avg       0.53      0.53      0.52     11524\n",
      "\n",
      "Model: CombinedNeuralNetwork\n",
      "Test Loss: 1.0710\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.47      0.06      0.10       701\n",
      "2. Very good       0.43      0.43      0.43      1769\n",
      "     3. Good       0.53      0.67      0.59      4226\n",
      "     4. Fair       0.54      0.53      0.54      3368\n",
      "     5. Poor       0.67      0.46      0.55      1460\n",
      "\n",
      "    accuracy                           0.53     11524\n",
      "   macro avg       0.53      0.43      0.44     11524\n",
      "weighted avg       0.53      0.53      0.52     11524\n",
      "\n",
      "Model: MixedActivationNetwork\n",
      "Test Loss: 1.1792\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.41      0.19      0.26       701\n",
      "2. Very good       0.40      0.47      0.43      1769\n",
      "     3. Good       0.55      0.55      0.55      4226\n",
      "     4. Fair       0.51      0.56      0.54      3368\n",
      "     5. Poor       0.61      0.51      0.55      1460\n",
      "\n",
      "    accuracy                           0.51     11524\n",
      "   macro avg       0.50      0.45      0.47     11524\n",
      "weighted avg       0.51      0.51      0.51     11524\n",
      "\n",
      "Model: ResNetRegression\n",
      "Test Loss: 1.0742\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.40      0.22      0.29       701\n",
      "2. Very good       0.46      0.33      0.39      1769\n",
      "     3. Good       0.54      0.68      0.60      4226\n",
      "     4. Fair       0.54      0.53      0.53      3368\n",
      "     5. Poor       0.63      0.55      0.59      1460\n",
      "\n",
      "    accuracy                           0.54     11524\n",
      "   macro avg       0.52      0.46      0.48     11524\n",
      "weighted avg       0.53      0.54      0.53     11524\n",
      "\n",
      "Model: DynamicRoutingNetwork\n",
      "Test Loss: 1.3461\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.38      0.26      0.31       701\n",
      "2. Very good       0.45      0.32      0.37      1769\n",
      "     3. Good       0.52      0.67      0.59      4226\n",
      "     4. Fair       0.52      0.48      0.50      3368\n",
      "     5. Poor       0.58      0.53      0.55      1460\n",
      "\n",
      "    accuracy                           0.52     11524\n",
      "   macro avg       0.49      0.45      0.46     11524\n",
      "weighted avg       0.51      0.52      0.51     11524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results summary\n",
    "for result in results:\n",
    "    print(f\"Model: {result['model_name']}\")\n",
    "    print(f\"Test Loss: {result['test_loss']:.4f}\")\n",
    "    print(f\"Classification Report:\\n{result['classification_report']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "- check that workflow choses epoch with lowest validation error!\n",
    "- vizualise some of the validation errors over epochs: make sure 20 epochs is sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
