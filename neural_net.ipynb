{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# new:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the data from Lino's preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/X.csv')\n",
    "y = pd.read_csv('data/y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76822 entries, 0 to 76821\n",
      "Columns: 495 entries, age to job_end_2019\n",
      "dtypes: bool(259), float64(236)\n",
      "memory usage: 157.3 MB\n",
      "\n",
      " y info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76822 entries, 0 to 76821\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   sphus   76822 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 600.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"X info:\")\n",
    "X.info()\n",
    "print(\"\\n y info:\")\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of               sphus\n",
       "0           5. Poor\n",
       "1           4. Fair\n",
       "2           4. Fair\n",
       "3           3. Good\n",
       "4      2. Very good\n",
       "...             ...\n",
       "76817       4. Fair\n",
       "76818       4. Fair\n",
       "76819       3. Good\n",
       "76820       4. Fair\n",
       "76821       3. Good\n",
       "\n",
       "[76822 rows x 1 columns]>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, need to convert this to numeric to make it into tensor (later will need to map it back to categorical to get meaningful answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sphus\n",
      "0          4\n",
      "1          3\n",
      "2          3\n",
      "3          2\n",
      "4          1\n",
      "...      ...\n",
      "76817      3\n",
      "76818      3\n",
      "76819      2\n",
      "76820      3\n",
      "76821      2\n",
      "\n",
      "[76822 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y['sphus'] = label_encoder.fit_transform(y['sphus'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6999947931582099 0.14999609486865742 0.15000911197313269\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# train is now 70% of the entire data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_ratio, random_state = 123)\n",
    "\n",
    "# test is now 15% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = test_ratio / (test_ratio + validation_ratio), random_state = 10117)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n = n_train + n_val + n_test\n",
    "\n",
    "print((n_train / n), (n_val / n), (n_test / n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs in train data\n",
      "No Infinities in train data\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in the training data\n",
    "if X_train.isna().any().any() or y_train.isna().any().any():\n",
    "    print(\"NaNs in train data\")\n",
    "else:\n",
    "    print(\"No NaNs in train data\")\n",
    "\n",
    "# Check for infinities in the training data\n",
    "if (X_train == np.inf).any().any() or (y_train == np.inf).any().any():\n",
    "    print(\"Infinities in train data\")\n",
    "else:\n",
    "    print(\"No Infinities in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute & scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTING:\n",
    "\n",
    "# create copies of original\n",
    "X_train_imputed = X_train.copy()\n",
    "X_test_imputed = X_test.copy()\n",
    "X_val_imputed = X_val.copy()\n",
    "\n",
    "# non numeric\n",
    "# Select non-numeric columns (booleans)\n",
    "non_numeric_columns_train = X_train_imputed.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_test = X_test_imputed.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_val = X_val_imputed.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "\n",
    "# Impute non-numeric columns with the string \"missing\"\n",
    "X_train_imputed[non_numeric_columns_train] = X_train_imputed[non_numeric_columns_train].fillna(\"missing\")\n",
    "X_test_imputed[non_numeric_columns_test] = X_test_imputed[non_numeric_columns_test].fillna(\"missing\")\n",
    "X_val_imputed[non_numeric_columns_val] = X_val_imputed[non_numeric_columns_val].fillna(\"missing\")\n",
    "\n",
    "\n",
    "# numeric\n",
    "# Select numeric columns\n",
    "numeric_columns_train = X_train_imputed.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_test = X_test_imputed.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_val = X_val_imputed.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Initialize the imputer for numeric columns\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation to numeric columns in training data (fit & transform)\n",
    "X_train_imputed[numeric_columns_train] = numeric_imputer.fit_transform(X_train_imputed[numeric_columns_train])\n",
    "\n",
    "# Apply imputation to numeric columns in testing / val data using the *same* imputer (only transform)\n",
    "X_test_imputed[numeric_columns_test] = numeric_imputer.transform(X_test_imputed[numeric_columns_test])\n",
    "X_val_imputed[numeric_columns_val] = numeric_imputer.transform(X_val_imputed[numeric_columns_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING\n",
    "\n",
    "# initialise\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training data (fit and transform)\n",
    "X_train_imputed_scaled = X_train_imputed.copy()\n",
    "X_train_imputed_scaled[numeric_columns_train] = scaler.fit_transform(X_train_imputed[numeric_columns_train])\n",
    "\n",
    "# Scale testing & validation data (only transform)\n",
    "X_test_inputed_scaled = X_test_imputed.copy()\n",
    "X_test_inputed_scaled[numeric_columns_test] = scaler.transform(X_test_imputed[numeric_columns_test])\n",
    "\n",
    "X_val_imputed_scaled = X_val_imputed.copy()\n",
    "X_val_imputed_scaled[numeric_columns_test] = scaler.transform(X_val_imputed[numeric_columns_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature DataFrames to NumPy arrays and then to tensors\n",
    "X_train_torch = torch.tensor(X_train_imputed_scaled.to_numpy().astype(np.float32))\n",
    "X_val_torch = torch.tensor(X_val_imputed_scaled.to_numpy().astype(np.float32))\n",
    "X_test_torch = torch.tensor(X_test_inputed_scaled.to_numpy().astype(np.float32))\n",
    "\n",
    "# Convert label Series to tensors and unsqueeze to add an extra dimension\n",
    "y_train_torch = torch.tensor(y_train.to_numpy().astype(np.float32)).unsqueeze(1)\n",
    "y_val_torch = torch.tensor(y_val.to_numpy().astype(np.float32)).unsqueeze(1)\n",
    "y_test_torch = torch.tensor(y_test.to_numpy().astype(np.float32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we're doing classification we do NOT need to scale the y-label series right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader instances\n",
    "\n",
    "# Ensure targets are Long tensors and 1D\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch.long().view(-1))\n",
    "val_dataset = TensorDataset(X_val_torch, y_val_torch.long().view(-1))\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch.long().view(-1))\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below: had errors with targets being out of bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # number of output classes required\n",
    "    num_classes = len(torch.unique(y_train_torch))\n",
    "\n",
    "    # Ensure all target values are within the valid range\n",
    "    assert torch.all((y_train_torch >= 0) & (y_train_torch < num_classes)), \"Train targets out of range\"\n",
    "    assert torch.all((y_val_torch >= 0) & (y_val_torch < num_classes)), \"Validation targets out of range\"\n",
    "    assert torch.all((y_test_torch >= 0) & (y_test_torch < num_classes)), \"Test targets out of range\"\n",
    "    print(\"All OK\")\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic linear, 3 layered (???) NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, hidden_size, output_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(input_features, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Model:\n",
    "\n",
    "- Batch normalization helps in normalising  inputs of each layer, reducing internal covariate shift. \n",
    "- Same as before: nonlinear ReLU activation function ensures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.layer1(x)))\n",
    "        x = self.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout model\n",
    "- after 1st activation layer, using a dropout rate of 50% to prevent overfitting - works by randomly deactivating half of the output units during training. \n",
    "- LeakyReLU: against issue of dying nodes (neurons) through small, positive gradient for negative inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(DropoutNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 500)\n",
    "        self.layer2 = nn.Linear(500, 100)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(100, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.layer2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combo Model\n",
    "\n",
    "- integrates both dropout regularization and batch normalization across three layers \n",
    "- intended to enhance training stability and prevent overfitting\n",
    "- employs Leaky ReLU as the activation function again to ensure consistent small gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(CombinedNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Moderate dropout\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.leaky_relu(self.bn1(self.layer1(x))))\n",
    "        x = self.dropout2(self.leaky_relu(self.bn2(self.layer2(x))))\n",
    "        x = self.leaky_relu(self.bn3(self.layer3(x)))\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Activation Functions\n",
    "\n",
    "- uses different activation functions at different layers to capture different types of nonlinear relationships at various levels of abstraction\n",
    "-  idea is to enable the model to learn more *diverse* representations by varying the transformations applied to inputs / intermediate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedActivationNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(MixedActivationNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.elu = nn.ELU()\n",
    "        self.output_layer = nn.Linear(64, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.leaky_relu(self.layer2(x))\n",
    "        x = self.elu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "This came from ChatGPT, I didn't know a ResNet before:\n",
    "\n",
    "\"Employs principles of a Residual Network (ResNet) architecture adapted for regression tasks. It is designed with an initial linear block to project input features into a 64-dimensional space, followed by a series of residual blocks that apply transformations and integrate the original input via skip connections to enhance learning without degradation, culminating with a final linear layer to produce a scalar output\"\n",
    "\n",
    "- main point: residual blocks enable deeper networks by adding shortcut connections that bypass one or more layers, aiming to address the problem of vanishing gradients in deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build module\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_features, output_features)\n",
    "        self.bn1 = nn.BatchNorm1d(output_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear2 = nn.Linear(output_features, output_features)\n",
    "        self.bn2 = nn.BatchNorm1d(output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.linear1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity  \n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# build model\n",
    "class ResNetRegression(nn.Module):\n",
    "    def __init__(self, input_features, block, num_blocks, output_features=1):\n",
    "        super(ResNetRegression, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.input_features = input_features\n",
    "        self.current_features = 64  # here is initial number of features\n",
    "        self.init_block = nn.Sequential(\n",
    "            nn.Linear(input_features, self.current_features),\n",
    "            nn.BatchNorm1d(self.current_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.blocks = self._make_layer(block, 64, num_blocks)\n",
    "        self.final_layer = nn.Linear(64, output_features)\n",
    "\n",
    "    def _make_layer(self, block, output_features, num_blocks):\n",
    "        layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(block(self.current_features, output_features))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_block(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Routing Network\n",
    "\n",
    "This was purely from ChatGPT again, but the ideas is a \"routing\" mechanism that dynamically decides how much of each layer's output should contribute to the next layer's input.\n",
    "\n",
    "- uses gating layers to selectively control the flow of information through the network by multiplying the output of a linear transformation of the input (x_transformed) by a gating signal (gate). \n",
    "- gating mechanism is a sigmoid function, which scales the linearly transformed input before passing it through gated layers\n",
    "- as before, residual blocks enable deeper networks by adding shortcut connections that bypass one or more layers, aiming to address the problem of vanishing gradients in deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build module \n",
    "class GatingLayer(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(GatingLayer, self).__init__()\n",
    "        self.layer = nn.Linear(input_features, output_features)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_features, output_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.layer(x)\n",
    "        gate = self.gate(x)\n",
    "        return x_transformed * gate\n",
    "\n",
    "# build model\n",
    "class DynamicRoutingNetwork(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(DynamicRoutingNetwork, self).__init__()\n",
    "        self.layer1 = GatingLayer(input_features, 128)\n",
    "        self.layer2 = GatingLayer(128, 64)\n",
    "        self.layer3 = GatingLayer(64, 64)\n",
    "        self.output_layer = nn.Linear(64, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize models\n",
    "\n",
    "input_features = X_train_torch.shape[1]\n",
    "hidden_size = 64  \n",
    "output_features = num_classes  \n",
    "\n",
    "# Initialize the neural network\n",
    "model = NeuralNetwork(input_features, hidden_size, output_features)\n",
    "\n",
    "basic_model = NeuralNetwork(input_features, hidden_size, output_features)\n",
    "basic_model.name = \"basic\"\n",
    "\n",
    "# deep_model = DeepNeuralNetwork(input_features=X_train_torch.shape[1], output_features=num_classes)\n",
    "# deep_model.name = \"Deep\"\n",
    "\n",
    "# dropout_model = DropoutNeuralNetwork(input_features=X_train_torch.shape[1])\n",
    "# dropout_model.name = \"Dropout\"\n",
    "\n",
    "# combo_model = CombinedNeuralNetwork(input_features=X_train_torch.shape[1])\n",
    "# combo_model.name = \"Combo\"\n",
    "\n",
    "# mixed_model = CombinedNeuralNetwork(input_features=X_train_torch.shape[1])\n",
    "# mixed_model.name = \"Mixed\"\n",
    "\n",
    "# ResNet_model = ResNetRegression(input_features=X_train_torch.shape[1], block=ResidualBlock, num_blocks=3)\n",
    "# ResNet_model.name = \"ResNet\"\n",
    "\n",
    "# routing_model = ResNetRegression(input_features=X_train_torch.shape[1], block=ResidualBlock, num_blocks=3)\n",
    "# routing_model.name = \"routing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write functions for full workflow\n",
    "\n",
    "### 1) A function to train the model over one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, loss_fn, optimizer):\n",
    "    model.train()  \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        \n",
    "        # Ensure targets are Long tensor and 1D\n",
    "        targets = targets.long().view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero previous gradients\n",
    "\n",
    "        # Forward pass: compute model output\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backward pass: compute gradient of loss wrt to model params\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Return the average loss for this epoch\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) A function to evaluate error on entire val / test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_loader, model, loss_fn):\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "\n",
    "            # Ensure targets are Long tensor and 1D\n",
    "            targets = targets.long().view(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Aggregate the loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    # Return average loss for the entire dataset\n",
    "    average_loss = total_loss / len(data_loader) \n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) A function to integrate training and validation, returning the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(train_loader, val_loader, model, epochs, optimizer, loss_fn):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train the model for one epoch\n",
    "        train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = evaluate_model(val_loader, model, loss_fn)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Check if this is the smallest so far and save associated model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) A function to get predictions from a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, _ in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            predictions.extend(predicted.cpu().numpy())  # Convert to numpy array and extend the list\n",
    "    \n",
    "    return np.array(predictions)  # Convert the list to a numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) A function to produce classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def generate_classification_report(model, test_loader):\n",
    "    predictions = get_predictions(model, test_loader)\n",
    "    \n",
    "    # Extract true labels from test_loader\n",
    "    true_labels = []\n",
    "    for _, targets in test_loader:\n",
    "        true_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Generate and print classification report\n",
    "    report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)\n",
    "    print(f\"Classification Report for {model.__class__.__name__}:\")\n",
    "    print(report)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) A final workflow function - brings it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow(models, train_loader, val_loader, test_loader, epochs=20):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    results = []\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"Training model: {model.__class__.__name__}\")\n",
    "        \n",
    "        # Define loss function and optimizer \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train and validate the model\n",
    "        trained_model, train_losses, val_losses = train_and_validate(\n",
    "            train_loader, val_loader, model, epochs, optimizer, loss_fn\n",
    "        )\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_loss = evaluate_model(test_loader, trained_model, loss_fn)\n",
    "        \n",
    "        classification_report_str = generate_classification_report(trained_model, test_loader)\n",
    "        \n",
    "        results.append({\n",
    "            'model_name': model.__class__.__name__,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'test_loss': test_loss,\n",
    "            'classification_report': classification_report_str\n",
    "        })\n",
    "        \n",
    "        # Track the best model based on validation loss\n",
    "        if min(val_losses) < best_val_loss:\n",
    "            best_val_loss = min(val_losses)\n",
    "            best_model = trained_model\n",
    "        \n",
    "        print(f\"Completed training for model: {model.__class__.__name__}\")\n",
    "\n",
    "    return best_model, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: need to provide following parameters to run_workflow():\n",
    "- models (a vector of models???), \n",
    "- train_loader, \n",
    "- val_loader, \n",
    "- test_loader, \n",
    "- (epochs)\n",
    "\n",
    "and it will return:\n",
    "- best_model,\n",
    "- results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Comparative Training & Validating\n",
    "\n",
    "Reminder: need to provide following parameters to run_workflow():\n",
    "Summary: need to provide following parameters to run_workflow():\n",
    "- models (a vector of models???), \n",
    "- train_loader, \n",
    "- val_loader, \n",
    "- test_loader, \n",
    "- (epochs)\n",
    "\n",
    "and it will return:\n",
    "- best_model,\n",
    "- results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model, test_results = run_workflow(deep_model, train_loader, val_loader, test_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: NeuralNetwork\n",
      "Epoch 1/20, Train Loss: 1.4221, Validation Loss: 1.4031\n",
      "Epoch 2/20, Train Loss: 1.3870, Validation Loss: 1.3787\n",
      "Epoch 3/20, Train Loss: 1.3718, Validation Loss: 1.3749\n",
      "Epoch 4/20, Train Loss: 1.3665, Validation Loss: 1.3723\n",
      "Epoch 5/20, Train Loss: 1.3631, Validation Loss: 1.3726\n",
      "Epoch 6/20, Train Loss: 1.3609, Validation Loss: 1.3706\n",
      "Epoch 7/20, Train Loss: 1.3593, Validation Loss: 1.3679\n",
      "Epoch 8/20, Train Loss: 1.3575, Validation Loss: 1.3737\n",
      "Epoch 9/20, Train Loss: 1.3567, Validation Loss: 1.3699\n",
      "Epoch 10/20, Train Loss: 1.3549, Validation Loss: 1.3686\n",
      "Epoch 11/20, Train Loss: 1.3535, Validation Loss: 1.3680\n",
      "Epoch 12/20, Train Loss: 1.3528, Validation Loss: 1.3708\n",
      "Epoch 13/20, Train Loss: 1.3520, Validation Loss: 1.3687\n",
      "Epoch 14/20, Train Loss: 1.3511, Validation Loss: 1.3672\n",
      "Epoch 15/20, Train Loss: 1.3498, Validation Loss: 1.3674\n",
      "Epoch 16/20, Train Loss: 1.3494, Validation Loss: 1.3691\n",
      "Epoch 17/20, Train Loss: 1.3490, Validation Loss: 1.3686\n",
      "Epoch 18/20, Train Loss: 1.3488, Validation Loss: 1.3684\n",
      "Epoch 19/20, Train Loss: 1.3478, Validation Loss: 1.3682\n",
      "Epoch 20/20, Train Loss: 1.3474, Validation Loss: 1.3725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrybaker/miniconda3/envs/MLLab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/henrybaker/miniconda3/envs/MLLab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/henrybaker/miniconda3/envs/MLLab/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for NeuralNetwork:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "1. Excellent       0.00      0.00      0.00       701\n",
      "2. Very good       0.43      0.40      0.42      1769\n",
      "     3. Good       0.53      0.62      0.57      4226\n",
      "     4. Fair       0.51      0.58      0.54      3368\n",
      "     5. Poor       0.65      0.50      0.57      1460\n",
      "\n",
      "    accuracy                           0.52     11524\n",
      "   macro avg       0.43      0.42      0.42     11524\n",
      "weighted avg       0.49      0.52      0.51     11524\n",
      "\n",
      "Completed training for model: NeuralNetwork\n",
      "Training model: NeuralNetwork\n",
      "Epoch 1/20, Train Loss: 1.4078, Validation Loss: 1.3888\n",
      "Epoch 2/20, Train Loss: 1.3757, Validation Loss: 1.3814\n",
      "Epoch 3/20, Train Loss: 1.3687, Validation Loss: 1.3746\n",
      "Epoch 4/20, Train Loss: 1.3663, Validation Loss: 1.3761\n",
      "Epoch 5/20, Train Loss: 1.3617, Validation Loss: 1.3830\n",
      "Epoch 6/20, Train Loss: 1.3598, Validation Loss: 1.3739\n",
      "Epoch 7/20, Train Loss: 1.3586, Validation Loss: 1.3696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     NeuralNetwork(input_features\u001b[38;5;241m=\u001b[39mX_train_torch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], output_features\u001b[38;5;241m=\u001b[39mnum_classes, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      4\u001b[0m     NeuralNetwork(input_features\u001b[38;5;241m=\u001b[39mX_train_torch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], output_features\u001b[38;5;241m=\u001b[39mnum_classes, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m      5\u001b[0m     NeuralNetwork(input_features\u001b[38;5;241m=\u001b[39mX_train_torch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], output_features\u001b[38;5;241m=\u001b[39mnum_classes, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run \u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m best_model, results \u001b[38;5;241m=\u001b[39m run_workflow(models, train_loader, val_loader, test_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Print results summary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[119], line 14\u001b[0m, in \u001b[0;36mrun_workflow\u001b[0;34m(models, train_loader, val_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train and validate the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m trained_model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m train_and_validate(\n\u001b[1;32m     15\u001b[0m     train_loader, val_loader, model, epochs, optimizer, loss_fn\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[1;32m     19\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m evaluate_model(test_loader, trained_model, loss_fn)\n",
      "Cell \u001b[0;32mIn[116], line 9\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(train_loader, val_loader, model, epochs, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      5\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Train the model for one epoch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(train_loader, model, loss_fn, optimizer)\n\u001b[1;32m     10\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[114], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_loader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Backward pass: compute gradient of loss wrt to model params\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Update the model parameters\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/MLLab/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/MLLab/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/MLLab/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example models\n",
    "models = [\n",
    "    NeuralNetwork(input_features=X_train_torch.shape[1], output_features=num_classes, hidden_size=10),\n",
    "    NeuralNetwork(input_features=X_train_torch.shape[1], output_features=num_classes, hidden_size=100),\n",
    "    NeuralNetwork(input_features=X_train_torch.shape[1], output_features=num_classes, hidden_size=1000)\n",
    "]\n",
    "\n",
    "# Run \n",
    "best_model, results = run_workflow(models, train_loader, val_loader, test_loader, epochs=20)\n",
    "\n",
    "# Print results summary\n",
    "for result in results:\n",
    "    print(f\"Model: {result['model_name']}\")\n",
    "    print(f\"Test Loss: {result['test_loss']:.4f}\")\n",
    "    print(f\"Classification Report:\\n{result['classification_report']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
