{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# new:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the data from Lino's preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TAKEN DIRECTLY FROM COMBINED SCRIPT:\n",
    "\n",
    "# easyshare = pd.read_stata('data/sharewX_rel8-0-0_easySHARE_stata/easySHARE_rel8-0-0.dta')\n",
    "\n",
    "# illness_before = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/illness_before_module_v01.dta\")\n",
    "# illness_during = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/illness_during_module_v01.dta\")\n",
    "# job = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/job_module_v01.dta\")\n",
    "# life = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/life_module_v01.dta\")\n",
    "# young_age = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/young_age_module_v01.dta\")\n",
    "# yearly = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/yearly_module_v01.dta\")\n",
    "\n",
    "# df = pd.merge(easyshare, life, on=['mergeid', 'wave'], how='left')\n",
    "# df = pd.merge(df, job, on=['mergeid'], how='left')\n",
    "\n",
    "# #_____\n",
    "\n",
    "# df_sorted = df.sort_values(by=['mergeid', 'wave'], ascending=[True, False])\n",
    "# df_most_recent_wave_per_mergeid = df_sorted.drop_duplicates(subset='mergeid', keep='first') # this is the full dataset, we should be trying to run models on this default\n",
    "\n",
    "# df_relevant = df_most_recent_wave_per_mergeid[df_most_recent_wave_per_mergeid.columns.drop(list(df.filter(regex='^euro')))]\n",
    "# df_relevant = df_relevant[df_relevant.columns.drop(list(df_relevant.filter(regex='^dn')))]\n",
    "# non_predictive_vars = [\n",
    "#     'mergeid',    # Used for merging records, no predictive power\n",
    "#     'hhid',       # Household identifier for tracking or grouping data\n",
    "#     'coupleid',   # Links records of individuals within a household\n",
    "#     'int_version',# Version of the questionnaire or interview format\n",
    "#     'int_year',   # Year the interview was conducted, structural rather than predictive\n",
    "#     'int_month',  # Month the interview was conducted, similar to int_year\n",
    "#     'country',    # Country code, used for stratification or adjustments\n",
    "#     'country_mod', # Modified country code, typically for data manipulation\n",
    "#     'wavepart'   # Wave part, used for stratification or adjustments\n",
    "# ]\n",
    "# df_relevant = df_relevant[df_relevant.columns.drop(non_predictive_vars)]\n",
    "\n",
    "# def replace_dash_with_na(df_relevant):\n",
    "#     for column in df_relevant.columns:\n",
    "#         if df_relevant[column].dtype == 'category':\n",
    "#             # Replace entries containing '-' with NA\n",
    "#             df_relevant[column] = df[column].apply(lambda x: pd.NA if '-' in str(x) else x)\n",
    "#     return df_relevant\n",
    "\n",
    "# df_relevant = replace_dash_with_na(df_relevant)\n",
    "\n",
    "# #_____\n",
    "\n",
    "# na_counts = df_relevant.groupby('wave').apply(lambda x: x.isnull().sum())\n",
    "# # mean per number of abservation per wave\n",
    "# na_counts['mean']= na_counts.mean(axis=1)\n",
    "# na_counts['obs'] = df_relevant.groupby('wave').size()\n",
    "# na_counts['avg_mean'] = na_counts['mean']/ na_counts['obs']\n",
    "# na_counts['std'] = na_counts.std(axis=1)\n",
    "# na_counts = na_counts.sort_values(by='avg_mean', ascending=False)\n",
    "# df_sorted = df_relevant.sort_values(by=['wave'], ascending=[False])\n",
    "# # drop all except wave 7 \n",
    "# df_wave_7 = df_sorted[df_sorted['wave'] == 7]\n",
    "# df_wave_7 = df_wave_7.drop(columns=['wave'])\n",
    "# # df_most_recent_wave_per_mergeid = df_sorted.drop_duplicates(subset='mergeid', keep='first')\n",
    "\n",
    "# na_counts = df_wave_7.isna().sum()\n",
    "\n",
    "# na_counts_sorted = na_counts.sort_values(ascending=False)\n",
    "\n",
    "# for column in df_wave_7.columns:\n",
    "#     if df_wave_7[column].dtype == object:  # Check if the column data type is object\n",
    "#         # Try converting the column to numeric\n",
    "#         converted_column = pd.to_numeric(df_wave_7[column], errors='coerce')\n",
    "#         # Check if the conversion did not introduce any new NaNs (i.e., all NaNs in the original are NaNs in the converted)\n",
    "#         if converted_column.notna().equals(df_wave_7[column].notna()):\n",
    "#             df_wave_7[column] = converted_column\n",
    "\n",
    "# columns_to_drop = na_counts[na_counts > 20000].index\n",
    "\n",
    "# df_dropped = df_wave_7.drop(columns=columns_to_drop)\n",
    "\n",
    "# na_after_dr = df_dropped.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# def process_categorical_columns(df):\n",
    "#     for column in df.columns:\n",
    "#         if df[column].dtype == 'category' or df[column].dtype == 'object':\n",
    "#             # Convert category to string\n",
    "#             df[column] = df[column].astype(str)\n",
    "\n",
    "#             # Split the column on the first '.', and expand to new DataFrame\n",
    "#             split_data = df[column].str.split('.', expand=True, n=1)\n",
    "\n",
    "#             # If split_data has only one column, no '.' was found; skip processing\n",
    "#             if split_data.shape[1] < 2:\n",
    "#                 continue\n",
    "\n",
    "#             # Clean up whitespace\n",
    "#             split_data[1] = split_data[1].str.strip()\n",
    "\n",
    "#             # Keep only rows where a split occurred (indicative of having a '.')\n",
    "#             valid_splits = split_data[1].notna()\n",
    "\n",
    "#             # Create dummy variables only for the valid name parts\n",
    "#             if valid_splits.any():\n",
    "#                 dummies = pd.get_dummies(split_data.loc[valid_splits, 1], prefix=column)\n",
    "#                 # Merge these dummy variables back to the original DataFrame\n",
    "#                 df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "#             # Optionally, drop the original column\n",
    "#             df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# df_final = process_categorical_columns(df_dropped)\n",
    "\n",
    "# df_dropped.info()\n",
    "# df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"df_dropped dtypes:\")\n",
    "# print(df_dropped.info())\n",
    "# #\n",
    "# print(\"\\ndf_final dtypes:\")\n",
    "# print(df_final.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/X.csv')\n",
    "y = pd.read_csv('data/y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76822 entries, 0 to 76821\n",
      "Columns: 495 entries, age to job_end_2019\n",
      "dtypes: bool(259), float64(236)\n",
      "memory usage: 157.3 MB\n",
      "\n",
      " y info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76822 entries, 0 to 76821\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   sphus   76822 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 600.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"X info:\")\n",
    "X.info()\n",
    "print(\"\\n y info:\")\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of               sphus\n",
       "0           5. Poor\n",
       "1           4. Fair\n",
       "2           4. Fair\n",
       "3           3. Good\n",
       "4      2. Very good\n",
       "...             ...\n",
       "76817       4. Fair\n",
       "76818       4. Fair\n",
       "76819       3. Good\n",
       "76820       4. Fair\n",
       "76821       3. Good\n",
       "\n",
       "[76822 rows x 1 columns]>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, need to convert this to numeric to make it into tensor (later will need to map it back to categorical to get meaningful answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sphus\n",
      "0          4\n",
      "1          3\n",
      "2          3\n",
      "3          2\n",
      "4          1\n",
      "...      ...\n",
      "76817      3\n",
      "76818      3\n",
      "76819      2\n",
      "76820      3\n",
      "76821      2\n",
      "\n",
      "[76822 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y['sphus'] = label_encoder.fit_transform(y['sphus'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here is new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6999947931582099 0.14999609486865742 0.15000911197313269\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# train is now 70% of the entire data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_ratio, random_state = 10117)\n",
    "\n",
    "# test is now 15% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = test_ratio / (test_ratio + validation_ratio), random_state = 10117)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n = n_train + n_val + n_test\n",
    "\n",
    "print((n_train / n), (n_val / n), (n_test / n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training it: first need to remove all NaNs / impute\n",
    "\n",
    "# Check for NaNs in the training data\n",
    "if X_train.isna().any().any() or y_train.isna().any().any():\n",
    "    print(\"NaNs in train data\")\n",
    "\n",
    "# Check for infinities in the training data\n",
    "if (X_train == np.inf).any().any() or (y_train == np.inf).any().any():\n",
    "    print(\"Infinities in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute & scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTING:\n",
    "\n",
    "# Select non-numeric columns (booleans)\n",
    "non_numeric_columns_train = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_test = X_test.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_val = X_val.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "# Impute non-numeric columns with the string \"missing\"\n",
    "X_train[non_numeric_columns_train] = X_train[non_numeric_columns_train].fillna(\"missing\")\n",
    "X_test[non_numeric_columns_test] = X_test[non_numeric_columns_test].fillna(\"missing\")\n",
    "X_val[non_numeric_columns_val] = X_val[non_numeric_columns_val].fillna(\"missing\")\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_columns_train = X_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_test = X_test.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_val = X_val.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Initialize the imputer for numeric columns\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation to numeric columns in training data (fit & transform)\n",
    "X_train[numeric_columns_train] = numeric_imputer.fit_transform(X_train[numeric_columns_train])\n",
    "\n",
    "# Apply imputation to numeric columns in testing / val data using the *same* imputer (only transform)\n",
    "X_test[numeric_columns_test] = numeric_imputer.transform(X_test[numeric_columns_test])\n",
    "X_val[numeric_columns_test] = numeric_imputer.transform(X_val[numeric_columns_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING\n",
    "\n",
    "# initialise\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training data (fit and transform)\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numeric_columns_train] = scaler.fit_transform(X_train[numeric_columns_train])\n",
    "\n",
    "# Scale testing data (only transform)\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numeric_columns_test] = scaler.transform(X_test[numeric_columns_test])\n",
    "\n",
    "# Scale validation data (only transform)\n",
    "X_val_scaled = X_val.copy()\n",
    "X_val_scaled[numeric_columns_test] = scaler.transform(X_val[numeric_columns_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature DataFrames to NumPy arrays and then to tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled.to_numpy().astype(np.float32))\n",
    "X_val_torch = torch.tensor(X_val_scaled.to_numpy().astype(np.float32))\n",
    "X_test_torch = torch.tensor(X_test_scaled.to_numpy().astype(np.float32))\n",
    "\n",
    "# Convert label Series to tensors and unsqueeze to add an extra dimension\n",
    "y_train_torch = torch.tensor(y_train.to_numpy().astype(np.float32)).unsqueeze(1)\n",
    "y_val_torch = torch.tensor(y_val.to_numpy().astype(np.float32)).unsqueeze(1)\n",
    "y_test_torch = torch.tensor(y_test.to_numpy().astype(np.float32)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DataLoader instances\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch) # makes them into joined Tensor\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # converts into a data loader, with 64 samples in a batch\n",
    "\n",
    "val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a training function:\n",
    "\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()  \n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "\n",
    "        # forward pass: compute model output\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # backward pass: compute  gradient of  loss wrt to model params\n",
    "        optimizer.zero_grad()  # zero prev gradients\n",
    "        loss.backward()  # backpropagation\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # for clipping gradients - needed later\n",
    "\n",
    "        # update the model parameters - NB: calling the step function on an Optimizer makes an update to its params\n",
    "        optimizer.step()\n",
    "\n",
    "        # update total loss and the batch count\n",
    "        total_loss += loss.item() * inputs.size(0) # this will depend on defined batchsize in dataloader?\n",
    "        num_samples +=inputs.size(0) \n",
    "\n",
    "    average_loss = total_loss / num_samples\n",
    "    print(f\"Average loss: {average_loss:.4f}\") # prints running avg\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to evaluate error on entire val / test set\n",
    "\n",
    "def evaluate_model(dataloader, model, loss_fn):\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient computation during evaluation\n",
    "        for inputs, targets in dataloader:\n",
    "\n",
    "            # forward pass: \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            # loss\n",
    "            loss = loss_fn(predictions, targets)\n",
    "\n",
    "            # aggregate the loss\n",
    "            total_loss += loss.item() * inputs.size(0)  \n",
    "            num_samples += inputs.size(0)\n",
    "            \n",
    "            # NB THE DIFFERENCE HERE IS NOT LEARNING NEW PARAMS IN EVAL MODE\n",
    "\n",
    "    average_loss = total_loss / num_samples  \n",
    "    print(f\"Average loss over evaluation data: {average_loss:.4f}\")\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining basic NN model\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden = nn.Linear(input_features, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  training model w/ diff sized hidden layers\n",
    "\n",
    "def run_training2(train_loader, val_loader, epochs, hidden_size):\n",
    "    \n",
    "    model = NeuralNetwork(input_features=X_train_torch.shape[1], hidden_size=hidden_size) # added this inside of the function no we want to try diff values - need to pass hidden_size now it has multiple layers\n",
    "    loss_fn = nn.MSELoss() # set these as fixed params\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # set these as fixed params\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
    "        val_loss = evaluate_model(val_loader, model, loss_fn)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "        \n",
    "\n",
    "# 10 hidden nodes\n",
    "print(\"Training with 10 hidden nodes:\")\n",
    "train_losses_10, val_losses_10 = run_training2(train_loader=train_loader, \n",
    "                                             val_loader=val_loader,\n",
    "                                             epochs = 20,\n",
    "                                             hidden_size=10)\n",
    "\n",
    "# 1000 hidden nodes\n",
    "print(\"Training with 1000 hidden nodes:\")\n",
    "train_losses_1000, val_losses_1000 = run_training2(train_loader=train_loader, \n",
    "                                             val_loader=val_loader,\n",
    "                                             epochs = 20,\n",
    "                                             hidden_size=1000)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses_10, label='Training Loss (10 Nodes)')\n",
    "plt.plot(val_losses_10, label='Validation Loss (10 Nodes)')\n",
    "plt.plot(train_losses_1000, label='Training Loss (1000 Nodes)')\n",
    "plt.plot(val_losses_1000, label='Validation Loss (1000 Nodes)')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training IN THIS CASE A LINEAR MODEL using Adam\n",
    "\n",
    "\n",
    "# step 1: get data ready SEE ABOVE\n",
    "\n",
    "\n",
    "# Step 2: Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "model = SimpleLinearRegression(input_features=X_train_torch.shape[1])\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "\n",
    "# Step 3: Define Training Functions\n",
    "\n",
    "def run_training(model, train_loader, val_loader, loss_fn, optimizer, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "      \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1) # decreasing step sizes as had issue with NaNs\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
    "        val_loss = evaluate_model(val_loader, model, loss_fn)\n",
    "        #scheduler.step()  # decreasing step sizes as had issue with NaNs\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "# Step 4: Training the Model and Collecting Errors\n",
    "\n",
    "num_epochs = 500  # NB aritrary\n",
    "train_losses, val_losses = run_training(model, train_loader, val_loader, loss_fn, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "# Step 5: Plotting  Learning Curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.axhline(y=mse_val, color='r', linestyle='--', label='Baseline Error (Validation MSE)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
