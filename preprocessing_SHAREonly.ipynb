{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:16.984339Z",
     "start_time": "2024-05-22T12:39:16.982024Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:18.322953Z",
     "start_time": "2024-05-22T12:39:17.033134Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_stata('data/sharewX_rel8-0-0_easySHARE_stata/easySHARE_rel8-0-0.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:18.389766Z",
     "start_time": "2024-05-22T12:39:18.324140Z"
    }
   },
   "outputs": [],
   "source": [
    "df_relevant = df[df.columns.drop(list(df.filter(regex='^euro')))]\n",
    "df_relevant = df_relevant[df_relevant.columns.drop(list(df_relevant.filter(regex='^dn')))]\n",
    "non_predictive_vars = [\n",
    "    'mergeid',    # Used for merging records, no predictive power\n",
    "    'hhid',       # Household identifier for tracking or grouping data\n",
    "    'coupleid',   # Links records of individuals within a household\n",
    "    'int_version',# Version of the questionnaire or interview format\n",
    "    'int_year',   # Year the interview was conducted, structural rather than predictive\n",
    "    'int_month',  # Month the interview was conducted, similar to int_year\n",
    "    'country',    # Country code, used for stratification or adjustments\n",
    "    'country_mod', # Modified country code, typically for data manipulation\n",
    "    'wavepart', # Wave part, used for stratification or adjustments\n",
    "    'recall_1',\n",
    "    'recall_2',   \n",
    "]\n",
    "df_relevant = df_relevant[df_relevant.columns.drop(non_predictive_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:18.801877Z",
     "start_time": "2024-05-22T12:39:18.390612Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_dash_with_na(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'category':\n",
    "            # Replace entries containing '-' with NA\n",
    "            df[column] = df[column].apply(lambda x: np.nan if '-' in str(x) else x)\n",
    "    return df\n",
    "\n",
    "df_relevant = replace_dash_with_na(df_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:19.847734Z",
     "start_time": "2024-05-22T12:39:18.803685Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k8/f_082ywn19j155v0vtbt4vbr0000gn/T/ipykernel_67304/2181989927.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  na_counts = df_relevant.groupby('wave').apply(lambda x: x.isnull().sum())\n"
     ]
    }
   ],
   "source": [
    "na_counts = df_relevant.groupby('wave').apply(lambda x: x.isnull().sum())\n",
    "# mean per number of abservation per wave\n",
    "na_counts['mean']= na_counts.mean(axis=1)\n",
    "na_counts['obs'] = df_relevant.groupby('wave').size()\n",
    "na_counts['avg_mean'] = na_counts['mean']/ na_counts['obs']\n",
    "na_counts['std'] = na_counts.std(axis=1)\n",
    "na_counts = na_counts.sort_values(by='avg_mean', ascending=False)\n",
    "df_sorted = df_relevant.sort_values(by=['wave'], ascending=[False])\n",
    "# drop all except wave 7 \n",
    "df_wave_7 = df_sorted[df_sorted['wave'] == 7]\n",
    "df_wave_7 = df_wave_7.drop(columns=['wave'])\n",
    "# df_most_recent_wave_per_mergeid = df_sorted.drop_duplicates(subset='mergeid', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:19.925959Z",
     "start_time": "2024-05-22T12:39:19.848738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "income_pct_w8    77202\nincome_pct_w1    77202\nbmi2             77202\nincome_pct_w6    77202\nincome_pct_w5    77202\n                 ...  \nfemale               0\nhhsize               0\npartnerinhh          0\nthinc_m              0\nlanguage             0\nLength: 79, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_counts = df_wave_7.isna().sum()\n",
    "\n",
    "na_counts_sorted = na_counts.sort_values(ascending=False)\n",
    "\n",
    "na_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.045821Z",
     "start_time": "2024-05-22T12:39:19.926886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Data columns (total 79 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   language          77202 non-null  category\n",
      " 1   female            77202 non-null  category\n",
      " 2   age               77195 non-null  float64 \n",
      " 3   birth_country     76907 non-null  object  \n",
      " 4   citizenship       77014 non-null  object  \n",
      " 5   iv009_mod         73361 non-null  object  \n",
      " 6   q34_re            0 non-null      float64 \n",
      " 7   isced1997_r       3372 non-null   object  \n",
      " 8   eduyears_mod      69931 non-null  float64 \n",
      " 9   mar_stat          77044 non-null  object  \n",
      " 10  hhsize            77202 non-null  category\n",
      " 11  partnerinhh       77202 non-null  category\n",
      " 12  int_partner       54783 non-null  object  \n",
      " 13  age_partner       54753 non-null  float64 \n",
      " 14  gender_partner    54783 non-null  object  \n",
      " 15  mother_alive      76968 non-null  object  \n",
      " 16  father_alive      76626 non-null  object  \n",
      " 17  siblings_alive    69309 non-null  float64 \n",
      " 18  ch001_            13722 non-null  float64 \n",
      " 19  ch021_mod         12424 non-null  float64 \n",
      " 20  ch007_hh          9016 non-null   object  \n",
      " 21  ch007_km          9045 non-null   object  \n",
      " 22  sp002_mod         13891 non-null  object  \n",
      " 23  sp003_1_mod       3258 non-null   object  \n",
      " 24  sp003_2_mod       1295 non-null   object  \n",
      " 25  sp003_3_mod       464 non-null    object  \n",
      " 26  sp008_            13884 non-null  object  \n",
      " 27  sp009_1_mod       3410 non-null   object  \n",
      " 28  sp009_2_mod       1035 non-null   object  \n",
      " 29  sp009_3_mod       326 non-null    object  \n",
      " 30  books_age10       3940 non-null   object  \n",
      " 31  maths_age10       61598 non-null  object  \n",
      " 32  language_age10    61643 non-null  object  \n",
      " 33  vaccinated        61970 non-null  object  \n",
      " 34  childhood_health  62621 non-null  object  \n",
      " 35  sphus             76822 non-null  object  \n",
      " 36  chronic_mod       76670 non-null  float64 \n",
      " 37  casp              70595 non-null  float64 \n",
      " 38  bfi10_extra_mod   74015 non-null  object  \n",
      " 39  bfi10_agree_mod   74132 non-null  object  \n",
      " 40  bfi10_consc_mod   74126 non-null  object  \n",
      " 41  bfi10_neuro_mod   74190 non-null  object  \n",
      " 42  bfi10_open_mod    73927 non-null  object  \n",
      " 43  hc002_mod         76248 non-null  object  \n",
      " 44  hc012_            76836 non-null  object  \n",
      " 45  hc029_            76100 non-null  object  \n",
      " 46  maxgrip           69132 non-null  float64 \n",
      " 47  adlwa             76159 non-null  float64 \n",
      " 48  adla              76159 non-null  float64 \n",
      " 49  iadla             76159 non-null  float64 \n",
      " 50  iadlza            76159 non-null  float64 \n",
      " 51  mobilityind       75880 non-null  float64 \n",
      " 52  lgmuscle          76531 non-null  float64 \n",
      " 53  grossmotor        75880 non-null  float64 \n",
      " 54  finemotor         75880 non-null  float64 \n",
      " 55  orienti           13921 non-null  object  \n",
      " 56  numeracy_1        454 non-null    object  \n",
      " 57  numeracy_2        13944 non-null  object  \n",
      " 58  bmi               74137 non-null  float64 \n",
      " 59  bmi2              0 non-null      float64 \n",
      " 60  smoking           10033 non-null  object  \n",
      " 61  ever_smoked       13923 non-null  object  \n",
      " 62  br010_mod         0 non-null      float64 \n",
      " 63  br015_            13937 non-null  object  \n",
      " 64  ep005_            58305 non-null  object  \n",
      " 65  ep009_mod         1041 non-null   object  \n",
      " 66  ep011_mod         171 non-null    object  \n",
      " 67  ep013_mod         1382 non-null   float64 \n",
      " 68  ep026_mod         1338 non-null   object  \n",
      " 69  ep036_mod         1388 non-null   object  \n",
      " 70  co007_            75685 non-null  object  \n",
      " 71  thinc_m           77202 non-null  float32 \n",
      " 72  income_pct_w1     0 non-null      float64 \n",
      " 73  income_pct_w2     0 non-null      float64 \n",
      " 74  income_pct_w4     0 non-null      float64 \n",
      " 75  income_pct_w5     0 non-null      float64 \n",
      " 76  income_pct_w6     0 non-null      float64 \n",
      " 77  income_pct_w7     13954 non-null  float64 \n",
      " 78  income_pct_w8     0 non-null      float64 \n",
      "dtypes: category(4), float32(1), float64(29), object(45)\n",
      "memory usage: 44.8+ MB\n"
     ]
    }
   ],
   "source": [
    "for column in df_wave_7.columns:\n",
    "    if df_wave_7[column].dtype == object:  # Check if the column data type is object\n",
    "        # Try converting the column to numeric\n",
    "        converted_column = pd.to_numeric(df_wave_7[column], errors='coerce')\n",
    "        # Check if the conversion did not introduce any new NaNs (i.e., all NaNs in the original are NaNs in the converted)\n",
    "        if converted_column.notna().equals(df_wave_7[column].notna()):\n",
    "            df_wave_7[column] = converted_column\n",
    "\n",
    "df_wave_7.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.061081Z",
     "start_time": "2024-05-22T12:39:21.046579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 77202\n",
      "No. of columns (full): 79\n",
      "No. of columns (dropped): 41\n"
     ]
    },
    {
     "data": {
      "text/plain": "Index(['q34_re', 'isced1997_r', 'int_partner', 'age_partner', 'gender_partner',\n       'ch001_', 'ch021_mod', 'ch007_hh', 'ch007_km', 'sp002_mod',\n       'sp003_1_mod', 'sp003_2_mod', 'sp003_3_mod', 'sp008_', 'sp009_1_mod',\n       'sp009_2_mod', 'sp009_3_mod', 'books_age10', 'orienti', 'numeracy_1',\n       'numeracy_2', 'bmi2', 'smoking', 'ever_smoked', 'br010_mod', 'br015_',\n       'ep009_mod', 'ep011_mod', 'ep013_mod', 'ep026_mod', 'ep036_mod',\n       'income_pct_w1', 'income_pct_w2', 'income_pct_w4', 'income_pct_w5',\n       'income_pct_w6', 'income_pct_w7', 'income_pct_w8'],\n      dtype='object')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = na_counts[na_counts > 20000].index\n",
    "\n",
    "\n",
    "df_dropped = df_wave_7.drop(columns=columns_to_drop)\n",
    "\n",
    "shape_of_dataframe_full = df_wave_7.shape\n",
    "shape_of_dataframe_dropped = df_dropped.shape\n",
    "\n",
    "print(f\"No. of samples: {shape_of_dataframe_full[0]}\")\n",
    "print(f\"No. of columns (full): {shape_of_dataframe_full[1]}\")\n",
    "print(f\"No. of columns (dropped): {shape_of_dataframe_dropped[1]}\")\n",
    "\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.102150Z",
     "start_time": "2024-05-22T12:39:21.061857Z"
    }
   },
   "outputs": [],
   "source": [
    "na_after_dr = df_dropped.isna().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.146470Z",
     "start_time": "2024-05-22T12:39:21.102821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Data columns (total 41 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   language          77202 non-null  category\n",
      " 1   female            77202 non-null  category\n",
      " 2   age               77195 non-null  float64 \n",
      " 3   birth_country     76907 non-null  object  \n",
      " 4   citizenship       77014 non-null  object  \n",
      " 5   iv009_mod         73361 non-null  object  \n",
      " 6   eduyears_mod      69931 non-null  float64 \n",
      " 7   mar_stat          77044 non-null  object  \n",
      " 8   hhsize            77202 non-null  category\n",
      " 9   partnerinhh       77202 non-null  category\n",
      " 10  mother_alive      76968 non-null  object  \n",
      " 11  father_alive      76626 non-null  object  \n",
      " 12  siblings_alive    69309 non-null  float64 \n",
      " 13  maths_age10       61598 non-null  object  \n",
      " 14  language_age10    61643 non-null  object  \n",
      " 15  vaccinated        61970 non-null  object  \n",
      " 16  childhood_health  62621 non-null  object  \n",
      " 17  sphus             76822 non-null  object  \n",
      " 18  chronic_mod       76670 non-null  float64 \n",
      " 19  casp              70595 non-null  float64 \n",
      " 20  bfi10_extra_mod   74015 non-null  object  \n",
      " 21  bfi10_agree_mod   74132 non-null  object  \n",
      " 22  bfi10_consc_mod   74126 non-null  object  \n",
      " 23  bfi10_neuro_mod   74190 non-null  object  \n",
      " 24  bfi10_open_mod    73927 non-null  object  \n",
      " 25  hc002_mod         76248 non-null  object  \n",
      " 26  hc012_            76836 non-null  object  \n",
      " 27  hc029_            76100 non-null  object  \n",
      " 28  maxgrip           69132 non-null  float64 \n",
      " 29  adlwa             76159 non-null  float64 \n",
      " 30  adla              76159 non-null  float64 \n",
      " 31  iadla             76159 non-null  float64 \n",
      " 32  iadlza            76159 non-null  float64 \n",
      " 33  mobilityind       75880 non-null  float64 \n",
      " 34  lgmuscle          76531 non-null  float64 \n",
      " 35  grossmotor        75880 non-null  float64 \n",
      " 36  finemotor         75880 non-null  float64 \n",
      " 37  bmi               74137 non-null  float64 \n",
      " 38  ep005_            58305 non-null  object  \n",
      " 39  co007_            75685 non-null  object  \n",
      " 40  thinc_m           77202 non-null  float32 \n",
      "dtypes: category(4), float32(1), float64(15), object(21)\n",
      "memory usage: 22.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.265309Z",
     "start_time": "2024-05-22T12:39:21.148837Z"
    }
   },
   "outputs": [],
   "source": [
    "df_relevant = df_dropped.dropna(subset=['sphus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.278848Z",
     "start_time": "2024-05-22T12:39:21.266312Z"
    }
   },
   "outputs": [],
   "source": [
    "# computing sphus into binary variable\n",
    "df_relevant.loc[df_relevant['sphus'] == '5. Poor', 'sphus'] = 1\n",
    "\n",
    "# Set the value to 0 where the condition is not met\n",
    "df_relevant.loc[df_relevant['sphus'] != 1, 'sphus'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally one hot encoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.956852Z",
     "start_time": "2024-05-22T12:39:21.279466Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assume df_relevant is your DataFrame and 'sphus' is the target variable\n",
    "X = df_relevant.drop('sphus', axis=1)  # Predictor variables\n",
    "y = df_relevant['sphus']\n",
    "\n",
    "# Convert all pd.NA in the DataFrame to np.nan\n",
    "X = X.replace({pd.NA: np.nan})\n",
    "def find_columns_with_string_and_numbers(df):\n",
    "    mixed_type_columns = []\n",
    "    for column in df.columns:\n",
    "        has_numeric = False\n",
    "        has_string = False\n",
    "        # Iterate over non-null items checking their type\n",
    "        for item in df[column].dropna():\n",
    "            if isinstance(item, str):\n",
    "                has_string = True\n",
    "            if isinstance(item, (int, float, np.number)):\n",
    "                has_numeric = True\n",
    "            # If both types are found, record the column and break the loop\n",
    "            if has_numeric and has_string:\n",
    "                mixed_type_columns.append(column)\n",
    "                break\n",
    "    return mixed_type_columns\n",
    "\n",
    "mixed_type_columns = find_columns_with_string_and_numbers(X)\n",
    "# drop mixed type columns\n",
    "X = X.drop(columns=mixed_type_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:21.960498Z",
     "start_time": "2024-05-22T12:39:21.957711Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_categorical_columns(df):\n",
    "    for column in df.columns:\n",
    "        if column != 'sphus':\n",
    "            if (df[column].dtype == 'category' or df[column].dtype == 'object') and df[column].nunique() < 100:\n",
    "                dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)\n",
    "                    # Merge these dummy variables back to the original DataFrame\n",
    "                df = pd.concat([df, dummies], axis=1)\n",
    "                df = df.drop(columns=column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:22.614560Z",
     "start_time": "2024-05-22T12:39:21.961364Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the transformation pipelines for both numerical and categorical data\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64', 'float32']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Pipelines for numeric and categorical transformations\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy= 'mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "X[numeric_features] = numeric_transformer.fit_transform(X[numeric_features])\n",
    "X[categorical_features] = categorical_transformer.fit_transform(X[categorical_features])\n",
    "\n",
    "X = process_categorical_columns(X)\n",
    "X = X.drop(columns= X.select_dtypes(include=['object', 'category']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:22.626411Z",
     "start_time": "2024-05-22T12:39:22.615603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "age                          0\nhhsize_2                     0\nhhsize_12                    0\nhhsize_11                    0\nhhsize_10                    0\n                            ..\nlanguage_28. Czech           0\nlanguage_27. Russian (il)    0\nlanguage_26. Arabic (il)     0\nlanguage_25. Hebrew (il)     0\nco007__4. Easily             0\nLength: 103, dtype: int64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show NAs\n",
    "X.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:24.711259Z",
     "start_time": "2024-05-22T12:39:22.627449Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving X and y\n",
    "X.to_csv('data/X_SHARE.csv', index=False)\n",
    "y.to_csv('data/y_SHARE.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HB comment: 10/06 \n",
    "wooooooo Lino this worked! i needed to do some minor encoding to get y to be in numeric format for tensor transformation (see NN script). \n",
    "I know the other models don't need this, but shall we include it in the overall transformation, then we can just create a dictionay mapping at the end to go back and forth between outputed numeric results and meaningful categorical ones? \n",
    "Edit: actually I think some of the imputing and scaling i do we'll want across all models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HB edits \n",
    "\n",
    "1) for some reason the neural net script gave different responses, so we'll need to adapt accordingly (so we treat this subsequent processing as a template)\n",
    "\n",
    "2) we should move these processing steps to before the X / y, test/train/val split so we're acting on the original df which we then can just export out as a csv and load directly into our other scripts as we're working on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data/processed_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:24.889017Z",
     "start_time": "2024-05-22T12:39:24.712253Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('sphus', axis=1) \n",
    "y = df['sphus'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:25.112544Z",
     "start_time": "2024-05-22T12:39:24.889880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 0.14999878673169784 0.15000121326830215\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# train is now 70% of the entire data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_ratio, random_state = 10117)\n",
    "\n",
    "# test is now 15% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = test_ratio / (test_ratio + validation_ratio), random_state = 10117)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n = n_train + n_val + n_test\n",
    "\n",
    "print((n_train / n), (n_val / n), (n_test / n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:25.430712Z",
     "start_time": "2024-05-22T12:39:25.113617Z"
    }
   },
   "outputs": [],
   "source": [
    "# training it: first need to remove all NaNs / impute\n",
    "\n",
    "# Check for NaNs in the training data\n",
    "if X_train.isna().any().any() or y_train.isna().any().any():\n",
    "    print(\"NaNs in train data\")\n",
    "\n",
    "# Check for infinities in the training data\n",
    "if (X_train == np.inf).any().any() or (y_train == np.inf).any().any():\n",
    "    print(\"Infinities in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute & Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:25.929081Z",
     "start_time": "2024-05-22T12:39:25.431673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select non-numeric columns (objects and booleans)\n",
    "non_numeric_columns_train = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_test = X_test.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_val = X_val.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "\n",
    "# Impute non-numeric columns with the string \"missing\"\n",
    "X_train[non_numeric_columns_train] = X_train[non_numeric_columns_train].fillna(\"missing\")\n",
    "X_test[non_numeric_columns_test] = X_test[non_numeric_columns_test].fillna(\"missing\")\n",
    "X_val[non_numeric_columns_val] = X_val[non_numeric_columns_val].fillna(\"missing\")\n",
    "\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_columns_train = X_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_test = X_test.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_val = X_val.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Initialize the imputer for numeric columns\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation to numeric columns in training data\n",
    "X_train[numeric_columns_train] = numeric_imputer.fit_transform(X_train[numeric_columns_train])\n",
    "\n",
    "# Apply imputation to numeric columns in testing data using the same imputer\n",
    "X_test[numeric_columns_test] = numeric_imputer.transform(X_test[numeric_columns_test])\n",
    "X_val[numeric_columns_test] = numeric_imputer.transform(X_val[numeric_columns_test])\n",
    "\n",
    "# initialise\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training data (fit and transform)\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numeric_columns_train] = scaler.fit_transform(X_train[numeric_columns_train])\n",
    "\n",
    "# Scale testing data (only transform)\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numeric_columns_test] = scaler.transform(X_test[numeric_columns_test])\n",
    "\n",
    "# Scale validation data (only transform)\n",
    "X_val_scaled = X_val.copy()\n",
    "X_val_scaled[numeric_columns_test] = scaler.transform(X_val[numeric_columns_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there's still some odd remaining floats  (that prevent me converting to Tensors), so deal with them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:25.934794Z",
     "start_time": "2024-05-22T12:39:25.930187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with dtype 'object':\n",
      "['mergeid', 'hhid', 'coupleid']\n",
      "\n",
      " values in each object column:\n",
      "\n",
      " Column: mergeid\n",
      "139010    DK-367119-03\n",
      "258621    GR-863675-01\n",
      "362486    PT-347341-02\n",
      "59849     CZ-013379-01\n",
      "34900     Bf-717782-02\n",
      "166174    EE-509767-01\n",
      "368040    RO-667384-02\n",
      "100039    Cg-666351-01\n",
      "222681    FI-933616-01\n",
      "104707    Ci-963123-01\n",
      "108745    DE-158453-01\n",
      "104985    DE-007604-01\n",
      "227162    FR-224531-01\n",
      "11060     AT-521280-01\n",
      "29305     Bf-355197-02\n",
      "389388    SE-846670-02\n",
      "256415    GR-758399-01\n",
      "343861    NL-879907-02\n",
      "222250    FI-794198-01\n",
      "169784    EE-645194-01\n",
      "7987      AT-383972-01\n",
      "228749    FR-322665-01\n",
      "186385    ES-330504-01\n",
      "365128    PT-871084-02\n",
      "281360    IT-225114-02\n",
      "1451      AT-065734-01\n",
      "13558     AT-636509-02\n",
      "333933    NL-293121-02\n",
      "389778    SE-864367-02\n",
      "63407     CZ-152255-01\n",
      "Name: mergeid, dtype: object\n",
      "\n",
      " Column: hhid\n",
      "139010    DK-367119-A\n",
      "258621    GR-863675-A\n",
      "362486    PT-347341-A\n",
      "59849     CZ-013379-A\n",
      "34900     Bf-717782-A\n",
      "166174    EE-509767-A\n",
      "368040    RO-667384-A\n",
      "100039    Cg-666351-A\n",
      "222681    FI-933616-A\n",
      "104707    Ci-963123-A\n",
      "108745    DE-158453-A\n",
      "104985    DE-007604-A\n",
      "227162    FR-224531-A\n",
      "11060     AT-521280-A\n",
      "29305     Bf-355197-A\n",
      "389388    SE-846670-A\n",
      "256415    GR-758399-A\n",
      "343861    NL-879907-A\n",
      "222250    FI-794198-A\n",
      "169784    EE-645194-A\n",
      "7987      AT-383972-A\n",
      "228749    FR-322665-A\n",
      "186385    ES-330504-A\n",
      "365128    PT-871084-A\n",
      "281360    IT-225114-A\n",
      "1451      AT-065734-A\n",
      "13558     AT-636509-A\n",
      "333933    NL-293121-A\n",
      "389778    SE-864367-A\n",
      "63407     CZ-152255-A\n",
      "Name: hhid, dtype: object\n",
      "\n",
      " Column: coupleid\n",
      "139010    DK-367119-01-03\n",
      "258621                   \n",
      "362486    PT-347341-01-02\n",
      "59849     CZ-013379-01-02\n",
      "34900     Bf-717782-01-02\n",
      "166174    EE-509767-01-02\n",
      "368040                   \n",
      "100039    Cg-666351-01-02\n",
      "222681    FI-933616-01-02\n",
      "104707                   \n",
      "108745    DE-158453-01-02\n",
      "104985    DE-007604-01-02\n",
      "227162    FR-224531-01-02\n",
      "11060                    \n",
      "29305     Bf-355197-01-02\n",
      "389388    SE-846670-01-02\n",
      "256415                   \n",
      "343861    NL-879907-01-02\n",
      "222250    FI-794198-01-04\n",
      "169784                   \n",
      "7987      AT-383972-01-02\n",
      "228749    FR-322665-01-02\n",
      "186385    ES-330504-01-02\n",
      "365128    PT-871084-01-02\n",
      "281360    IT-225114-01-02\n",
      "1451      AT-065734-01-02\n",
      "13558     AT-636509-01-02\n",
      "333933    NL-293121-01-02\n",
      "389778    SE-864367-01-02\n",
      "63407     CZ-152255-01-02\n",
      "Name: coupleid, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# dtypes of all columns\n",
    "column_dtypes = X_train_scaled.dtypes\n",
    "\n",
    "# only columns with dtype 'object'\n",
    "object_columns = column_dtypes[column_dtypes == 'object'].index.tolist()\n",
    "\n",
    "print(\"Columns with dtype 'object':\")\n",
    "print(object_columns)\n",
    "\n",
    "print(\"\\n values in each object column:\")\n",
    "\n",
    "for column in object_columns:\n",
    "    print(f\"\\n Column: {column}\")\n",
    "    print(X_train_scaled[column].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB THIS WAS DIFFERENT WHEN I RAN IT IN THE NEURAL NET SCRIPT... SO WE'LL NEED TO ADAPT THE BELOW CODES..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:27.410553Z",
     "start_time": "2024-05-22T12:39:25.935673Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NUTS2_floods'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/ML-Labs/lib/python3.12/site-packages/pandas/core/indexes/base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:153\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:182\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'NUTS2_floods'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m X_train_scaled[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcitizenship\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_train_scaled[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcitizenship\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mstr[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Process 'NUTS2_floods' by replacing '.' or '0' with 'missing'\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m X_train_scaled[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNUTS2_floods\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_train_scaled[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNUTS2_floods\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mreplace([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmissing\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpdated \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbirth_country\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m column:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(X_train_scaled[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbirth_country\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m))\n",
      "File \u001B[0;32m~/miniconda3/envs/ML-Labs/lib/python3.12/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/miniconda3/envs/ML-Labs/lib/python3.12/site-packages/pandas/core/indexes/base.py:3809\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3805\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3806\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3807\u001B[0m     ):\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3809\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3810\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3811\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3812\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'NUTS2_floods'"
     ]
    }
   ],
   "source": [
    "# Process 'birth_country' and 'citizenship' by removing the numeric prefix\n",
    "X_train_scaled['birth_country'] = X_train_scaled['birth_country'].str.split('. ').str[1]\n",
    "X_train_scaled['citizenship'] = X_train_scaled['citizenship'].str.split('. ').str[1]\n",
    "\n",
    "# Process 'NUTS2_floods' by replacing '.' or '0' with 'missing'\n",
    "X_train_scaled['NUTS2_floods'] = X_train_scaled['NUTS2_floods'].replace(['.', '0'], 'missing')\n",
    "\n",
    "print(\"Updated 'birth_country' column:\")\n",
    "print(X_train_scaled['birth_country'].head(10))\n",
    "\n",
    "print(\"\\nUpdated 'citizenship' column:\")\n",
    "print(X_train_scaled['citizenship'].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now X_train is sorted, still having issues with y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:47.148213Z",
     "start_time": "2024-05-22T12:39:47.144078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Excellent', '4. Fair', '3. Good', '5. Poor', '2. Very good', '-12. don't know / refusal', '-15. no information']\n",
      "Categories (7, object): ['-15. no information' < '-12. don't know / refusal' < '1. Excellent' < '2. Very good' < '3. Good' < '4. Fair' < '5. Poor']\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:50.040599Z",
     "start_time": "2024-05-22T12:39:49.436194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Excellent' 'Fair' 'Good' 'Poor' 'Very good' \"don't know / refusal\"\n",
      " 'no information']\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.str.extract(r'\\d+\\.\\s*(.*)')[0].copy()\n",
    "y_test = y_test.str.extract(r'\\d+\\.\\s*(.*)')[0].copy()\n",
    "\n",
    "y_train = y_train.replace(np.nan, \"missing\").copy()\n",
    "y_test = y_test.replace(np.nan, \"missing\").copy()\n",
    "\n",
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now need to use label encoding to run most models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:52.997179Z",
     "start_time": "2024-05-22T12:39:52.919112Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# initialise\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# fit and transform y_train to numeric labels\n",
    "y_train = encoder.fit_transform(y_train.fillna('missing'))  \n",
    "y_test = encoder.transform(y_test.fillna('missing'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) the problematic columns are different in this script that the neural net script... we'll need to adapt it accordngly, but they provide a good template\n",
    "\n",
    "2) we should move these processing steps to before the X / y, test/train/val split so we're acting on the original df which we then can just export out as a csv and load directly into our other scripts as we're working on them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
