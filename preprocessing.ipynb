{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "easyshare = pd.read_stata('data/sharewX_rel8-0-0_easySHARE_stata/easySHARE_rel8-0-0.dta')\n",
    "\n",
    "illness_before = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/illness_before_module_v01.dta\")\n",
    "illness_during = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/illness_during_module_v01.dta\")\n",
    "job = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/job_module_v01.dta\")\n",
    "life = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/life_module_v01.dta\")\n",
    "young_age = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/young_age_module_v01.dta\")\n",
    "yearly = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/yearly_module_v01.dta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(easyshare, life, on=['mergeid', 'wave'], how='left')\n",
    "df = pd.merge(df, job, on=['mergeid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant = df[df.columns.drop(list(df.filter(regex='^euro')))]\n",
    "df_relevant = df_relevant[df_relevant.columns.drop(list(df_relevant.filter(regex='^dn')))]\n",
    "non_predictive_vars = [\n",
    "    'mergeid',    # Used for merging records, no predictive power\n",
    "    'hhid',       # Household identifier for tracking or grouping data\n",
    "    'coupleid',   # Links records of individuals within a household\n",
    "    'int_version',# Version of the questionnaire or interview format\n",
    "    'int_year',   # Year the interview was conducted, structural rather than predictive\n",
    "    'int_month',  # Month the interview was conducted, similar to int_year\n",
    "    'country',    # Country code, used for stratification or adjustments\n",
    "    'country_mod', # Modified country code, typically for data manipulation\n",
    "    'wavepart', # Wave part, used for stratification or adjustments\n",
    "    'recall_1',\n",
    "    'recall_2',   \n",
    "]\n",
    "df_relevant = df_relevant[df_relevant.columns.drop(non_predictive_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dash_with_na(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'category':\n",
    "            # Replace entries containing '-' with NA\n",
    "            df[column] = df[column].apply(lambda x: pd.NA if '-' in str(x) else x)\n",
    "    return df\n",
    "\n",
    "df_relevant = replace_dash_with_na(df_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = df_relevant.groupby('wave').apply(lambda x: x.isnull().sum())\n",
    "# mean per number of abservation per wave\n",
    "na_counts['mean']= na_counts.mean(axis=1)\n",
    "na_counts['obs'] = df_relevant.groupby('wave').size()\n",
    "na_counts['avg_mean'] = na_counts['mean']/ na_counts['obs']\n",
    "na_counts['std'] = na_counts.std(axis=1)\n",
    "na_counts = na_counts.sort_values(by='avg_mean', ascending=False)\n",
    "df_sorted = df_relevant.sort_values(by=['wave'], ascending=[False])\n",
    "# drop all except wave 7 \n",
    "df_wave_7 = df_sorted[df_sorted['wave'] == 7]\n",
    "df_wave_7 = df_wave_7.drop(columns=['wave'])\n",
    "# df_most_recent_wave_per_mergeid = df_sorted.drop_duplicates(subset='mergeid', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emi_pm2p5_w      77202\n",
       "bmi2             77202\n",
       "emi_pm10_w       77202\n",
       "income_pct_w8    77202\n",
       "income_pct_w6    77202\n",
       "                 ...  \n",
       "thinc_m              0\n",
       "hhsize               0\n",
       "partnerinhh          0\n",
       "female               0\n",
       "language             0\n",
       "Length: 413, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_counts = df_wave_7.isna().sum()\n",
    "\n",
    "na_counts_sorted = na_counts.sort_values(ascending=False)\n",
    "\n",
    "na_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Columns: 413 entries, language to yjob_tx_g30_w\n",
      "dtypes: category(7), float32(309), float64(50), object(47)\n",
      "memory usage: 149.2+ MB\n"
     ]
    }
   ],
   "source": [
    "for column in df_wave_7.columns:\n",
    "    if df_wave_7[column].dtype == object:  # Check if the column data type is object\n",
    "        # Try converting the column to numeric\n",
    "        converted_column = pd.to_numeric(df_wave_7[column], errors='coerce')\n",
    "        # Check if the conversion did not introduce any new NaNs (i.e., all NaNs in the original are NaNs in the converted)\n",
    "        if converted_column.notna().equals(df_wave_7[column].notna()):\n",
    "            df_wave_7[column] = converted_column\n",
    "\n",
    "df_wave_7.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 77202\n",
      "No. of columns (full): 413\n",
      "No. of columns (dropped): 266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['q34_re', 'isced1997_r', 'int_partner', 'age_partner', 'gender_partner',\n",
       "       'ch001_', 'ch021_mod', 'ch007_hh', 'ch007_km', 'sp002_mod',\n",
       "       ...\n",
       "       'avgjob_conc_yearly_o3_mean', 'avgjob_conc_yearly_o3_median',\n",
       "       'avgjob_conc_yearly_o3_w', 'avgjob_emissions_PM10_mean',\n",
       "       'avgjob_emissions_PM10_median', 'avgjob_emissions_PM10_w',\n",
       "       'avgjob_emissions_PM25_mean', 'avgjob_emissions_PM25_median',\n",
       "       'avgjob_emissions_PM25_w', 'job_uncomfortable'],\n",
       "      dtype='object', length=147)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = na_counts[na_counts > 20000].index\n",
    "\n",
    "\n",
    "df_dropped = df_wave_7.drop(columns=columns_to_drop)\n",
    "\n",
    "shape_of_dataframe_full = df_wave_7.shape\n",
    "shape_of_dataframe_dropped = df_dropped.shape\n",
    "\n",
    "print(f\"No. of samples: {shape_of_dataframe_full[0]}\")\n",
    "print(f\"No. of columns (full): {shape_of_dataframe_full[1]}\")\n",
    "print(f\"No. of columns (dropped): {shape_of_dataframe_dropped[1]}\")\n",
    "\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_after_dr = df_dropped.isna().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Columns: 266 entries, language to yjob_tx_g30_w\n",
      "dtypes: category(7), float32(221), float64(15), object(23)\n",
      "memory usage: 88.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_dropped.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally one hot encoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical_columns(df):\n",
    "    for column in df.columns:\n",
    "        if column != 'sphus':\n",
    "            if (df[column].dtype == 'category' or df[column].dtype == 'object') and df[column].nunique() < 100:\n",
    "                dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)\n",
    "                    # Merge these dummy variables back to the original DataFrame\n",
    "                df = pd.concat([df, dummies], axis=1)\n",
    "                df = df.drop(columns=column)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = process_categorical_columns(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Columns: 730 entries, age to job_start_2017\n",
      "dtypes: bool(490), float32(221), float64(15), object(4)\n",
      "memory usage: 112.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "categorical = df_processed.select_dtypes(include='category').columns.tolist()\n",
    "print(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv('data/processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HB edits \n",
    "\n",
    "1) for some reason the neural net script gave different responses, so we'll need to adapt accordingly (so we treat this subsequent processing as a template)\n",
    "\n",
    "2) we should move these processing steps to before the X / y, test/train/val split so we're acting on the original df which we then can just export out as a csv and load directly into our other scripts as we're working on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data/processed_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('sphus', axis=1) \n",
    "y = df['sphus'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 0.14999878673169784 0.15000121326830215\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.70\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# train is now 70% of the entire data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_ratio, random_state = 10117)\n",
    "\n",
    "# test is now 15% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = test_ratio / (test_ratio + validation_ratio), random_state = 10117)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "n = n_train + n_val + n_test\n",
    "\n",
    "print((n_train / n), (n_val / n), (n_test / n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in train data\n"
     ]
    }
   ],
   "source": [
    "# training it: first need to remove all NaNs / impute\n",
    "\n",
    "# Check for NaNs in the training data\n",
    "if X_train.isna().any().any() or y_train.isna().any().any():\n",
    "    print(\"NaNs in train data\")\n",
    "\n",
    "# Check for infinities in the training data\n",
    "if (X_train == np.inf).any().any() or (y_train == np.inf).any().any():\n",
    "    print(\"Infinities in train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute & Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select non-numeric columns (objects and booleans)\n",
    "non_numeric_columns_train = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_test = X_test.select_dtypes(include=['object', 'bool']).columns\n",
    "non_numeric_columns_val = X_val.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "\n",
    "# Impute non-numeric columns with the string \"missing\"\n",
    "X_train[non_numeric_columns_train] = X_train[non_numeric_columns_train].fillna(\"missing\")\n",
    "X_test[non_numeric_columns_test] = X_test[non_numeric_columns_test].fillna(\"missing\")\n",
    "X_val[non_numeric_columns_val] = X_val[non_numeric_columns_val].fillna(\"missing\")\n",
    "\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_columns_train = X_train.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_test = X_test.select_dtypes(include=[np.number]).columns\n",
    "numeric_columns_val = X_val.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Initialize the imputer for numeric columns\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation to numeric columns in training data\n",
    "X_train[numeric_columns_train] = numeric_imputer.fit_transform(X_train[numeric_columns_train])\n",
    "\n",
    "# Apply imputation to numeric columns in testing data using the same imputer\n",
    "X_test[numeric_columns_test] = numeric_imputer.transform(X_test[numeric_columns_test])\n",
    "X_val[numeric_columns_test] = numeric_imputer.transform(X_val[numeric_columns_test])\n",
    "\n",
    "# initialise\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale training data (fit and transform)\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[numeric_columns_train] = scaler.fit_transform(X_train[numeric_columns_train])\n",
    "\n",
    "# Scale testing data (only transform)\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[numeric_columns_test] = scaler.transform(X_test[numeric_columns_test])\n",
    "\n",
    "# Scale validation data (only transform)\n",
    "X_val_scaled = X_val.copy()\n",
    "X_val_scaled[numeric_columns_test] = scaler.transform(X_val[numeric_columns_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there's still some odd remaining floats  (that prevent me converting to Tensors), so deal with them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with dtype 'object':\n",
      "['mergeid', 'hhid', 'coupleid', 'NUTS1_floods', 'NUTS2_floods']\n",
      "\n",
      " values in each object column:\n",
      "\n",
      " Column: mergeid\n",
      "139010    DK-367119-03\n",
      "258621    GR-863675-01\n",
      "362486    PT-347341-02\n",
      "59849     CZ-013379-01\n",
      "34900     Bf-717782-02\n",
      "166174    EE-509767-01\n",
      "368040    RO-667384-02\n",
      "100039    Cg-666351-01\n",
      "222681    FI-933616-01\n",
      "104707    Ci-963123-01\n",
      "108745    DE-158453-01\n",
      "104985    DE-007604-01\n",
      "227162    FR-224531-01\n",
      "11060     AT-521280-01\n",
      "29305     Bf-355197-02\n",
      "389388    SE-846670-02\n",
      "256415    GR-758399-01\n",
      "343861    NL-879907-02\n",
      "222250    FI-794198-01\n",
      "169784    EE-645194-01\n",
      "7987      AT-383972-01\n",
      "228749    FR-322665-01\n",
      "186385    ES-330504-01\n",
      "365128    PT-871084-02\n",
      "281360    IT-225114-02\n",
      "1451      AT-065734-01\n",
      "13558     AT-636509-02\n",
      "333933    NL-293121-02\n",
      "389778    SE-864367-02\n",
      "63407     CZ-152255-01\n",
      "Name: mergeid, dtype: object\n",
      "\n",
      " Column: hhid\n",
      "139010    DK-367119-A\n",
      "258621    GR-863675-A\n",
      "362486    PT-347341-A\n",
      "59849     CZ-013379-A\n",
      "34900     Bf-717782-A\n",
      "166174    EE-509767-A\n",
      "368040    RO-667384-A\n",
      "100039    Cg-666351-A\n",
      "222681    FI-933616-A\n",
      "104707    Ci-963123-A\n",
      "108745    DE-158453-A\n",
      "104985    DE-007604-A\n",
      "227162    FR-224531-A\n",
      "11060     AT-521280-A\n",
      "29305     Bf-355197-A\n",
      "389388    SE-846670-A\n",
      "256415    GR-758399-A\n",
      "343861    NL-879907-A\n",
      "222250    FI-794198-A\n",
      "169784    EE-645194-A\n",
      "7987      AT-383972-A\n",
      "228749    FR-322665-A\n",
      "186385    ES-330504-A\n",
      "365128    PT-871084-A\n",
      "281360    IT-225114-A\n",
      "1451      AT-065734-A\n",
      "13558     AT-636509-A\n",
      "333933    NL-293121-A\n",
      "389778    SE-864367-A\n",
      "63407     CZ-152255-A\n",
      "Name: hhid, dtype: object\n",
      "\n",
      " Column: coupleid\n",
      "139010    DK-367119-01-03\n",
      "258621                   \n",
      "362486    PT-347341-01-02\n",
      "59849     CZ-013379-01-02\n",
      "34900     Bf-717782-01-02\n",
      "166174    EE-509767-01-02\n",
      "368040                   \n",
      "100039    Cg-666351-01-02\n",
      "222681    FI-933616-01-02\n",
      "104707                   \n",
      "108745    DE-158453-01-02\n",
      "104985    DE-007604-01-02\n",
      "227162    FR-224531-01-02\n",
      "11060                    \n",
      "29305     Bf-355197-01-02\n",
      "389388    SE-846670-01-02\n",
      "256415                   \n",
      "343861    NL-879907-01-02\n",
      "222250    FI-794198-01-04\n",
      "169784                   \n",
      "7987      AT-383972-01-02\n",
      "228749    FR-322665-01-02\n",
      "186385    ES-330504-01-02\n",
      "365128    PT-871084-01-02\n",
      "281360    IT-225114-01-02\n",
      "1451      AT-065734-01-02\n",
      "13558     AT-636509-01-02\n",
      "333933    NL-293121-01-02\n",
      "389778    SE-864367-01-02\n",
      "63407     CZ-152255-01-02\n",
      "Name: coupleid, dtype: object\n",
      "\n",
      " Column: NUTS1_floods\n",
      "139010    missing\n",
      "258621          .\n",
      "362486        PT1\n",
      "59849         CZ0\n",
      "34900         BE3\n",
      "166174    missing\n",
      "368040    missing\n",
      "100039          .\n",
      "222681    missing\n",
      "104707    missing\n",
      "108745        DEE\n",
      "104985        DE9\n",
      "227162        FR8\n",
      "11060         AT3\n",
      "29305         BE3\n",
      "389388          .\n",
      "256415        EL5\n",
      "343861    missing\n",
      "222250    missing\n",
      "169784    missing\n",
      "7987          AT1\n",
      "228749        FR1\n",
      "186385        ES5\n",
      "365128        PT1\n",
      "281360        ITI\n",
      "1451          AT1\n",
      "13558           .\n",
      "333933    missing\n",
      "389778        SE3\n",
      "63407         CZ0\n",
      "Name: NUTS1_floods, dtype: object\n",
      "\n",
      " Column: NUTS2_floods\n",
      "139010    missing\n",
      "258621          .\n",
      "362486       PT11\n",
      "59849        CZ07\n",
      "34900         BE3\n",
      "166174    missing\n",
      "368040    missing\n",
      "100039          .\n",
      "222681    missing\n",
      "104707    missing\n",
      "108745        DEE\n",
      "104985        DE9\n",
      "227162        FR8\n",
      "11060        AT31\n",
      "29305         BE3\n",
      "389388          .\n",
      "256415       EL52\n",
      "343861    missing\n",
      "222250    missing\n",
      "169784    missing\n",
      "7987         AT13\n",
      "228749        FR1\n",
      "186385       ES52\n",
      "365128       PT11\n",
      "281360       ITI1\n",
      "1451         AT13\n",
      "13558           .\n",
      "333933    missing\n",
      "389778       SE32\n",
      "63407        CZ02\n",
      "Name: NUTS2_floods, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# dtypes of all columns\n",
    "column_dtypes = X_train_scaled.dtypes\n",
    "\n",
    "# only columns with dtype 'object'\n",
    "object_columns = column_dtypes[column_dtypes == 'object'].index.tolist()\n",
    "\n",
    "print(\"Columns with dtype 'object':\")\n",
    "print(object_columns)\n",
    "\n",
    "print(\"\\n values in each object column:\")\n",
    "\n",
    "for column in object_columns:\n",
    "    print(f\"\\n Column: {column}\")\n",
    "    print(X_train_scaled[column].head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB THIS WAS DIFFERENT WHEN I RAN IT IN THE NEURAL NET SCRIPT... SO WE'LL NEED TO ADAPT THE BELOW CODES..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'birth_country' column:\n",
      "139010        Denmark\n",
      "258621         Greece\n",
      "362486       Portugal\n",
      "59849            Czec\n",
      "34900         Belgium\n",
      "166174        Estonia\n",
      "368040        Romania\n",
      "100039    Switzerland\n",
      "222681        Finland\n",
      "104707          Italy\n",
      "Name: birth_country, dtype: object\n",
      "\n",
      "Updated 'citizenship' column:\n",
      "139010        Danish\n",
      "258621         Greek\n",
      "362486    Portuguese\n",
      "59849          Czech\n",
      "34900        Belgian\n",
      "166174      Estonian\n",
      "368040      Romanian\n",
      "100039         Swiss\n",
      "222681       Finnish\n",
      "104707         Swiss\n",
      "Name: citizenship, dtype: object\n",
      "\n",
      "Updated 'NUTS2_floods' column:\n",
      "139010    missing\n",
      "258621    missing\n",
      "362486       PT11\n",
      "59849        CZ07\n",
      "34900         BE3\n",
      "166174    missing\n",
      "368040    missing\n",
      "100039    missing\n",
      "222681    missing\n",
      "104707    missing\n",
      "Name: NUTS2_floods, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Process 'birth_country' and 'citizenship' by removing the numeric prefix\n",
    "X_train_scaled['birth_country'] = X_train_scaled['birth_country'].str.split('. ').str[1]\n",
    "X_train_scaled['citizenship'] = X_train_scaled['citizenship'].str.split('. ').str[1]\n",
    "\n",
    "# Process 'NUTS2_floods' by replacing '.' or '0' with 'missing'\n",
    "X_train_scaled['NUTS2_floods'] = X_train_scaled['NUTS2_floods'].replace(['.', '0'], 'missing')\n",
    "\n",
    "print(\"Updated 'birth_country' column:\")\n",
    "print(X_train_scaled['birth_country'].head(10))\n",
    "\n",
    "print(\"\\nUpdated 'citizenship' column:\")\n",
    "print(X_train_scaled['citizenship'].head(10))\n",
    "\n",
    "print(\"\\nUpdated 'NUTS2_floods' column:\")\n",
    "print(X_train_scaled['NUTS2_floods'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now X_train is sorted, still having issues with y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Excellent', '4. Fair', '3. Good', '5. Poor', '2. Very good', '-12. don't know / refusal', '-15. no information']\n",
      "Categories (7, object): ['-15. no information' < '-12. don't know / refusal' < '1. Excellent' < '2. Very good' < '3. Good' < '4. Fair' < '5. Poor']\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Excellent' 'Fair' 'Good' 'Poor' 'Very good' \"don't know / refusal\"\n",
      " 'no information']\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.str.extract(r'\\d+\\.\\s*(.*)')[0].copy()\n",
    "\n",
    "y_train = y_train.replace(np.nan, \"missing\").copy()\n",
    "\n",
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) the problematic columns are different in this script that the neural net script... we'll need to adapt it accordngly, but they provide a good template\n",
    "\n",
    "2) we should move these processing steps to before the X / y, test/train/val split so we're acting on the original df which we then can just export out as a csv and load directly into our other scripts as we're working on them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
