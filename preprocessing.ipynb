{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "easyshare = pd.read_stata('data/sharewX_rel8-0-0_easySHARE_stata/easySHARE_rel8-0-0.dta')\n",
    "\n",
    "illness_before = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/illness_before_module_v01.dta\")\n",
    "illness_during = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/illness_during_module_v01.dta\")\n",
    "job = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/job_module_v01.dta\")\n",
    "life = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/life_module_v01.dta\")\n",
    "young_age = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/young_age_module_v01.dta\")\n",
    "yearly = pd.read_stata(\"data/SHARE-ENV - Exposure to Environmental Hazards/yearly_module_v01.dta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(easyshare, life, on=['mergeid', 'wave'], how='left')\n",
    "df = pd.merge(df, job, on=['mergeid'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relevant = df[df.columns.drop(list(df.filter(regex='^euro')))]\n",
    "df_relevant = df_relevant[df_relevant.columns.drop(list(df_relevant.filter(regex='^dn')))]\n",
    "non_predictive_vars = [\n",
    "    'mergeid',    # Used for merging records, no predictive power\n",
    "    'hhid',       # Household identifier for tracking or grouping data\n",
    "    'coupleid',   # Links records of individuals within a household\n",
    "    'int_version',# Version of the questionnaire or interview format\n",
    "    'int_year',   # Year the interview was conducted, structural rather than predictive\n",
    "    'int_month',  # Month the interview was conducted, similar to int_year\n",
    "    'country',    # Country code, used for stratification or adjustments\n",
    "    'country_mod', # Modified country code, typically for data manipulation\n",
    "    'wavepart', # Wave part, used for stratification or adjustments\n",
    "    'recall_1',\n",
    "    'recall_2',   \n",
    "]\n",
    "df_relevant = df_relevant[df_relevant.columns.drop(non_predictive_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dash_with_na(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'category':\n",
    "            # Replace entries containing '-' with NA\n",
    "            df[column] = df[column].apply(lambda x: pd.NA if '-' in str(x) else x)\n",
    "    return df\n",
    "\n",
    "df_relevant = replace_dash_with_na(df_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/1wqkklmd4qzbv02gbc6344wc0000gn/T/ipykernel_48740/594858994.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  na_counts = df_relevant.groupby('wave').apply(lambda x: x.isnull().sum())\n"
     ]
    }
   ],
   "source": [
    "na_counts = df_relevant.groupby('wave').apply(lambda x: x.isnull().sum())\n",
    "# mean per number of abservation per wave\n",
    "na_counts['mean']= na_counts.mean(axis=1)\n",
    "na_counts['obs'] = df_relevant.groupby('wave').size()\n",
    "na_counts['avg_mean'] = na_counts['mean']/ na_counts['obs']\n",
    "na_counts['std'] = na_counts.std(axis=1)\n",
    "na_counts = na_counts.sort_values(by='avg_mean', ascending=False)\n",
    "df_sorted = df_relevant.sort_values(by=['wave'], ascending=[False])\n",
    "# drop all except wave 7 \n",
    "df_wave_7 = df_sorted[df_sorted['wave'] == 7]\n",
    "df_wave_7 = df_wave_7.drop(columns=['wave'])\n",
    "# df_most_recent_wave_per_mergeid = df_sorted.drop_duplicates(subset='mergeid', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emi_pm2p5_w      77202\n",
       "bmi2             77202\n",
       "emi_pm10_w       77202\n",
       "income_pct_w8    77202\n",
       "income_pct_w6    77202\n",
       "                 ...  \n",
       "thinc_m              0\n",
       "hhsize               0\n",
       "partnerinhh          0\n",
       "female               0\n",
       "language             0\n",
       "Length: 413, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_counts = df_wave_7.isna().sum()\n",
    "\n",
    "na_counts_sorted = na_counts.sort_values(ascending=False)\n",
    "\n",
    "na_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Columns: 413 entries, language to yjob_tx_g30_w\n",
      "dtypes: category(7), float32(309), float64(50), object(47)\n",
      "memory usage: 149.2+ MB\n"
     ]
    }
   ],
   "source": [
    "for column in df_wave_7.columns:\n",
    "    if df_wave_7[column].dtype == object:  # Check if the column data type is object\n",
    "        # Try converting the column to numeric\n",
    "        converted_column = pd.to_numeric(df_wave_7[column], errors='coerce')\n",
    "        # Check if the conversion did not introduce any new NaNs (i.e., all NaNs in the original are NaNs in the converted)\n",
    "        if converted_column.notna().equals(df_wave_7[column].notna()):\n",
    "            df_wave_7[column] = converted_column\n",
    "\n",
    "df_wave_7.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 77202\n",
      "No. of columns (full): 413\n",
      "No. of columns (dropped): 266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['q34_re', 'isced1997_r', 'int_partner', 'age_partner', 'gender_partner',\n",
       "       'ch001_', 'ch021_mod', 'ch007_hh', 'ch007_km', 'sp002_mod',\n",
       "       ...\n",
       "       'avgjob_conc_yearly_o3_mean', 'avgjob_conc_yearly_o3_median',\n",
       "       'avgjob_conc_yearly_o3_w', 'avgjob_emissions_PM10_mean',\n",
       "       'avgjob_emissions_PM10_median', 'avgjob_emissions_PM10_w',\n",
       "       'avgjob_emissions_PM25_mean', 'avgjob_emissions_PM25_median',\n",
       "       'avgjob_emissions_PM25_w', 'job_uncomfortable'],\n",
       "      dtype='object', length=147)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = na_counts[na_counts > 20000].index\n",
    "\n",
    "\n",
    "df_dropped = df_wave_7.drop(columns=columns_to_drop)\n",
    "\n",
    "shape_of_dataframe_full = df_wave_7.shape\n",
    "shape_of_dataframe_dropped = df_dropped.shape\n",
    "\n",
    "print(f\"No. of samples: {shape_of_dataframe_full[0]}\")\n",
    "print(f\"No. of columns (full): {shape_of_dataframe_full[1]}\")\n",
    "print(f\"No. of columns (dropped): {shape_of_dataframe_dropped[1]}\")\n",
    "\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_after_dr = df_dropped.isna().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Columns: 266 entries, language to yjob_tx_g30_w\n",
      "dtypes: category(7), float32(221), float64(15), object(23)\n",
      "memory usage: 88.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_dropped.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally one hot encoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical_columns(df):\n",
    "    for column in df.columns:\n",
    "        if column != 'sphus':\n",
    "            if (df[column].dtype == 'category' or df[column].dtype == 'object') and df[column].nunique() < 100:\n",
    "                dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)\n",
    "                    # Merge these dummy variables back to the original DataFrame\n",
    "                df = pd.concat([df, dummies], axis=1)\n",
    "                df = df.drop(columns=column)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = process_categorical_columns(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77202 entries, 22153 to 32326\n",
      "Columns: 730 entries, age to job_start_2017\n",
      "dtypes: bool(490), float32(221), float64(15), object(4)\n",
      "memory usage: 112.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "categorical = df_processed.select_dtypes(include='category').columns.tolist()\n",
    "print(categorical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
